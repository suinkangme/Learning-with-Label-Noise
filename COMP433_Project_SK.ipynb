{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suinkangme/comp433_project/blob/main/COMP433_Project_SK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Developing a robust CNN model to address the challenge of learning with label noise in  CIFAR10 dataset\n",
        "\n",
        "- CIFAR10 Label : ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’.\n",
        "\n",
        "- image size : 3x32x32\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TyD3ZYpdMfXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import os\n",
        "from sklearn.mixture import GaussianMixture"
      ],
      "metadata": {
        "id": "ItN3Ur88Mbn1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and normalize CIFAR10\n",
        "\n",
        "- Transform them to Tensors of normalized range [-1, 1]."
      ],
      "metadata": {
        "id": "Mby_AXQNk9k_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train dataset\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "cifar10_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)"
      ],
      "metadata": {
        "id": "em4BZ8EAmHGn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b40413f-6738-4990-e88e-f4f4ee5f4796"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# validation, test dataset\n",
        "test_val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])"
      ],
      "metadata": {
        "id": "CwdaaOWA5fSa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the dataset into train and validation\n",
        "train_size = int(0.8 * len(cifar10_dataset))\n",
        "val_size = len(cifar10_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(cifar10_dataset, [train_size, val_size])"
      ],
      "metadata": {
        "id": "laHDmghE6YLN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.dataset.transform = train_transform\n",
        "val_dataset.dataset.transform = test_val_transform\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "4HUffu9C6q3T"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_val_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rekxxb1b6YQM",
        "outputId": "51ef2828-4cb3-45c5-f10b-37d94e4ce488"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Noise Labeling\n",
        "- 5 different noise levels (10%,\n",
        "30%, 50%, 80%, 90%)"
      ],
      "metadata": {
        "id": "-U2PfzQbqjq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_label_noise(labels, epsilon, noise_type):\n",
        "    num_labels = len(labels)\n",
        "    num_flips = int(epsilon * num_labels)\n",
        "\n",
        "    if noise_type == 'symmetric':\n",
        "        # Symmetric label noise\n",
        "        flip_indices = np.random.choice(num_labels, num_flips, replace=False)\n",
        "        labels[flip_indices] = np.random.randint(0, 10, num_flips)\n",
        "    elif noise_type == 'asymmetric':\n",
        "        # Asymmetric label noise\n",
        "        flip_rules = {\n",
        "            9: 1,   # Truck to Automobile\n",
        "            2: 0,   # Bird to Airplane\n",
        "            4: 7,   # Deer to Horse\n",
        "            3: 5,   # Cat to Dog\n",
        "            5: 3,   # Dog to Cat\n",
        "        }\n",
        "\n",
        "        for i in range(num_labels):\n",
        "            if np.random.random() < epsilon:\n",
        "                labels[i] = flip_rules.get(labels[i], labels[i])\n",
        "\n",
        "    return labels"
      ],
      "metadata": {
        "id": "tyxVnHB5k2a5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Baseline Model**"
      ],
      "metadata": {
        "id": "qrSJVpxjNj2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a  baseline CNN model"
      ],
      "metadata": {
        "id": "cqkWDzlPmd8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaselineModel, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 8, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(8, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # fc layers\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Linear(128 * 8 * 8, 120),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(84, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc_layers(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "kGhXTfeRmeLA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train & Validation and Testing - symmetric noise label"
      ],
      "metadata": {
        "id": "F5kl1vfP7mjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# noise_levels\n",
        "noise_levels = [0.1, 0.3, 0.5, 0.8, 0.9]\n",
        "\n",
        "# create a dictionary with keys in the format 'noise_level_{100 * value}'\n",
        "model_dict = {f'noise_level_{int(100 * level)}_sy': None for level in noise_levels}\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# training\n",
        "for epsilon in noise_levels:\n",
        "\n",
        "    num_epochs = 20\n",
        "\n",
        "    model = BaselineModel()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    print(f\"Symmetric Training with noise level: {epsilon}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            # add symmetric noise to labels\n",
        "            labels = apply_label_noise(labels.numpy(), epsilon=epsilon, noise_type='symmetric')\n",
        "            labels = torch.from_numpy(labels)\n",
        "\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_accuracy = correct / total\n",
        "    average_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    print(f'Validation Loss: {average_val_loss}, Validation Accuracy: {val_accuracy}')\n",
        "\n",
        "    # save model to dictionary\n",
        "    model_dict[f'noise_level_{int(100 * epsilon)}_sy'] = {\n",
        "        'state_dict': model.state_dict(),\n",
        "        'validation_loss': average_val_loss,\n",
        "        'validation_accuracy': val_accuracy\n",
        "    }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lna2HUsA7JS5",
        "outputId": "f1c945c2-abcb-4da4-b2d1-a0152a32d511"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symmetric Training with noise level: 0.1\n",
            "Epoch 1/20, Loss: 1.578172206878662\n",
            "Epoch 2/20, Loss: 1.0516657829284668\n",
            "Epoch 3/20, Loss: 0.8020777106285095\n",
            "Epoch 4/20, Loss: 1.1523828506469727\n",
            "Epoch 5/20, Loss: 0.9929617643356323\n",
            "Epoch 6/20, Loss: 1.02053701877594\n",
            "Epoch 7/20, Loss: 1.0013731718063354\n",
            "Epoch 8/20, Loss: 0.984472393989563\n",
            "Epoch 9/20, Loss: 0.6997935771942139\n",
            "Epoch 10/20, Loss: 0.8753437995910645\n",
            "Epoch 11/20, Loss: 0.7374206781387329\n",
            "Epoch 12/20, Loss: 0.9731400012969971\n",
            "Epoch 13/20, Loss: 0.6782114505767822\n",
            "Epoch 14/20, Loss: 0.7411990165710449\n",
            "Epoch 15/20, Loss: 0.5805302262306213\n",
            "Epoch 16/20, Loss: 0.5508726239204407\n",
            "Epoch 17/20, Loss: 0.6559940576553345\n",
            "Epoch 18/20, Loss: 0.6445692181587219\n",
            "Epoch 19/20, Loss: 0.6315118074417114\n",
            "Epoch 20/20, Loss: 0.5050042271614075\n",
            "Validation Loss: 0.9203241023288411, Validation Accuracy: 0.7219\n",
            "Symmetric Training with noise level: 0.3\n",
            "Epoch 1/20, Loss: 1.8265252113342285\n",
            "Epoch 2/20, Loss: 1.638409972190857\n",
            "Epoch 3/20, Loss: 1.7427191734313965\n",
            "Epoch 4/20, Loss: 1.7763047218322754\n",
            "Epoch 5/20, Loss: 1.5321985483169556\n",
            "Epoch 6/20, Loss: 1.5534052848815918\n",
            "Epoch 7/20, Loss: 1.4261058568954468\n",
            "Epoch 8/20, Loss: 1.5548194646835327\n",
            "Epoch 9/20, Loss: 1.391169548034668\n",
            "Epoch 10/20, Loss: 1.5469833612442017\n",
            "Epoch 11/20, Loss: 1.4134852886199951\n",
            "Epoch 12/20, Loss: 1.4594786167144775\n",
            "Epoch 13/20, Loss: 1.4912179708480835\n",
            "Epoch 14/20, Loss: 1.5269098281860352\n",
            "Epoch 15/20, Loss: 1.5945940017700195\n",
            "Epoch 16/20, Loss: 1.4014827013015747\n",
            "Epoch 17/20, Loss: 1.3983001708984375\n",
            "Epoch 18/20, Loss: 1.3414971828460693\n",
            "Epoch 19/20, Loss: 1.275686264038086\n",
            "Epoch 20/20, Loss: 1.2459795475006104\n",
            "Validation Loss: 1.0267064487858184, Validation Accuracy: 0.7144\n",
            "Symmetric Training with noise level: 0.5\n",
            "Epoch 1/20, Loss: 2.0843348503112793\n",
            "Epoch 2/20, Loss: 2.1384804248809814\n",
            "Epoch 3/20, Loss: 1.9997663497924805\n",
            "Epoch 4/20, Loss: 2.0456204414367676\n",
            "Epoch 5/20, Loss: 2.0012335777282715\n",
            "Epoch 6/20, Loss: 2.0081264972686768\n",
            "Epoch 7/20, Loss: 1.9649935960769653\n",
            "Epoch 8/20, Loss: 1.9775511026382446\n",
            "Epoch 9/20, Loss: 1.9656567573547363\n",
            "Epoch 10/20, Loss: 2.103360414505005\n",
            "Epoch 11/20, Loss: 1.9505870342254639\n",
            "Epoch 12/20, Loss: 2.022214412689209\n",
            "Epoch 13/20, Loss: 1.8579236268997192\n",
            "Epoch 14/20, Loss: 1.811102032661438\n",
            "Epoch 15/20, Loss: 1.9527183771133423\n",
            "Epoch 16/20, Loss: 1.7794156074523926\n",
            "Epoch 17/20, Loss: 1.8703817129135132\n",
            "Epoch 18/20, Loss: 1.8056695461273193\n",
            "Epoch 19/20, Loss: 1.9061466455459595\n",
            "Epoch 20/20, Loss: 1.888117790222168\n",
            "Validation Loss: 1.2617513579168138, Validation Accuracy: 0.7003\n",
            "Symmetric Training with noise level: 0.8\n",
            "Epoch 1/20, Loss: 2.3149948120117188\n",
            "Epoch 2/20, Loss: 2.291133165359497\n",
            "Epoch 3/20, Loss: 2.2651360034942627\n",
            "Epoch 4/20, Loss: 2.252741575241089\n",
            "Epoch 5/20, Loss: 2.2770659923553467\n",
            "Epoch 6/20, Loss: 2.299485445022583\n",
            "Epoch 7/20, Loss: 2.2671799659729004\n",
            "Epoch 8/20, Loss: 2.2747135162353516\n",
            "Epoch 9/20, Loss: 2.2733757495880127\n",
            "Epoch 10/20, Loss: 2.2749838829040527\n",
            "Epoch 11/20, Loss: 2.2407479286193848\n",
            "Epoch 12/20, Loss: 2.244039297103882\n",
            "Epoch 13/20, Loss: 2.3274264335632324\n",
            "Epoch 14/20, Loss: 2.273466110229492\n",
            "Epoch 15/20, Loss: 2.2818198204040527\n",
            "Epoch 16/20, Loss: 2.280546188354492\n",
            "Epoch 17/20, Loss: 2.2812232971191406\n",
            "Epoch 18/20, Loss: 2.3015365600585938\n",
            "Epoch 19/20, Loss: 2.2468085289001465\n",
            "Epoch 20/20, Loss: 2.2518234252929688\n",
            "Validation Loss: 1.9505977547092803, Validation Accuracy: 0.5301\n",
            "Symmetric Training with noise level: 0.9\n",
            "Epoch 1/20, Loss: 2.3056721687316895\n",
            "Epoch 2/20, Loss: 2.3080883026123047\n",
            "Epoch 3/20, Loss: 2.3017828464508057\n",
            "Epoch 4/20, Loss: 2.3033766746520996\n",
            "Epoch 5/20, Loss: 2.3016016483306885\n",
            "Epoch 6/20, Loss: 2.2995188236236572\n",
            "Epoch 7/20, Loss: 2.303529977798462\n",
            "Epoch 8/20, Loss: 2.3057193756103516\n",
            "Epoch 9/20, Loss: 2.3028852939605713\n",
            "Epoch 10/20, Loss: 2.3012421131134033\n",
            "Epoch 11/20, Loss: 2.3017327785491943\n",
            "Epoch 12/20, Loss: 2.3054239749908447\n",
            "Epoch 13/20, Loss: 2.30159592628479\n",
            "Epoch 14/20, Loss: 2.300872802734375\n",
            "Epoch 15/20, Loss: 2.302299976348877\n",
            "Epoch 16/20, Loss: 2.3017303943634033\n",
            "Epoch 17/20, Loss: 2.3040401935577393\n",
            "Epoch 18/20, Loss: 2.301175832748413\n",
            "Epoch 19/20, Loss: 2.3053805828094482\n",
            "Epoch 20/20, Loss: 2.3023459911346436\n",
            "Validation Loss: 2.3025024240943277, Validation Accuracy: 0.1019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "for key, model_state_info in model_dict.items():\n",
        "\n",
        "    model = BaselineModel()\n",
        "    model_state = model.state_dict()\n",
        "    model_state.update({k: v for k, v in model_state_info['state_dict'].items() if k in model_state})\n",
        "\n",
        "    # load the updated state_dict\n",
        "    model.load_state_dict(model_state)\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Test Accuracy for {key}: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7Uv61au89vx",
        "outputId": "c870b8dc-86e7-4b03-b47f-e7caadcaf930"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy for noise_level_10_sy: 0.7219\n",
            "Test Accuracy for noise_level_30_sy: 0.7144\n",
            "Test Accuracy for noise_level_50_sy: 0.7003\n",
            "Test Accuracy for noise_level_80_sy: 0.5301\n",
            "Test Accuracy for noise_level_90_sy: 0.1019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train & Validation and Testing - asymmetric noise label"
      ],
      "metadata": {
        "id": "MP5bB-Jn716l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# noise_levels\n",
        "noise_levels = [0.1, 0.3, 0.5, 0.8, 0.9]\n",
        "\n",
        "# create a dictionary with keys in the format 'noise_level_{100 * value}'\n",
        "model_dict = {f'noise_level_{int(100 * level)}_asy': None for level in noise_levels}\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# training\n",
        "for epsilon in noise_levels:\n",
        "\n",
        "    num_epochs = 20\n",
        "\n",
        "    model = BaselineModel()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    print(f\"Asymmetric Training with noise level: {epsilon}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            # add symmetric noise to labels\n",
        "            labels = apply_label_noise(labels.numpy(), epsilon=epsilon, noise_type='asymmetric')\n",
        "            labels = torch.from_numpy(labels)\n",
        "\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_accuracy = correct / total\n",
        "    average_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    print(f'Validation Loss: {average_val_loss}, Validation Accuracy: {val_accuracy}')\n",
        "\n",
        "    # save model to dictionary\n",
        "    model_dict[f'noise_level_{int(100 * epsilon)}_asy'] = {\n",
        "        'state_dict': model.state_dict(),\n",
        "        'validation_loss': average_val_loss,\n",
        "        'validation_accuracy': val_accuracy\n",
        "    }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_nb_gK97boB",
        "outputId": "e7313e0a-3690-4393-fcd5-ab00842a50fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Asymmetric Training with noise level: 0.1\n",
            "Epoch 1/5, Loss: 1.3520103693008423\n",
            "Epoch 2/5, Loss: 0.9156202077865601\n",
            "Epoch 3/5, Loss: 0.5246793627738953\n",
            "Epoch 4/5, Loss: 0.8600916862487793\n",
            "Epoch 5/5, Loss: 0.8814508318901062\n",
            "Validation Loss: 0.8582057041727054, Validation Accuracy: 0.7022\n",
            "Asymmetric Training with noise level: 0.3\n",
            "Epoch 1/5, Loss: 1.460185170173645\n",
            "Epoch 2/5, Loss: 1.3619500398635864\n",
            "Epoch 3/5, Loss: 1.2083697319030762\n",
            "Epoch 4/5, Loss: 1.1095333099365234\n",
            "Epoch 5/5, Loss: 0.8080011010169983\n",
            "Validation Loss: 0.9474637424869902, Validation Accuracy: 0.6811\n",
            "Asymmetric Training with noise level: 0.5\n",
            "Epoch 1/5, Loss: 1.181923270225525\n",
            "Epoch 2/5, Loss: 1.2102376222610474\n",
            "Epoch 3/5, Loss: 0.9125158786773682\n",
            "Epoch 4/5, Loss: 0.8314177393913269\n",
            "Epoch 5/5, Loss: 0.9430887699127197\n",
            "Validation Loss: 1.1435436705115494, Validation Accuracy: 0.5394\n",
            "Asymmetric Training with noise level: 0.8\n",
            "Epoch 1/5, Loss: 1.2077199220657349\n",
            "Epoch 2/5, Loss: 1.123086929321289\n",
            "Epoch 3/5, Loss: 0.8528462648391724\n",
            "Epoch 4/5, Loss: 0.9869452118873596\n",
            "Epoch 5/5, Loss: 0.9469321966171265\n",
            "Validation Loss: 1.3520483678313577, Validation Accuracy: 0.4545\n",
            "Asymmetric Training with noise level: 0.9\n",
            "Epoch 1/5, Loss: 1.0520929098129272\n",
            "Epoch 2/5, Loss: 0.8389816284179688\n",
            "Epoch 3/5, Loss: 0.8570188879966736\n",
            "Epoch 4/5, Loss: 0.6321858763694763\n",
            "Epoch 5/5, Loss: 0.5614811778068542\n",
            "Validation Loss: 1.6428276968609756, Validation Accuracy: 0.4398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "for key, model_state_info in model_dict.items():\n",
        "\n",
        "    model = BaselineModel()\n",
        "    model_state = model.state_dict()\n",
        "    model_state.update({k: v for k, v in model_state_info['state_dict'].items() if k in model_state})\n",
        "\n",
        "    # load the updated state_dict\n",
        "    model.load_state_dict(model_state)\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Test Accuracy for {key}: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nb4dLto0wU3",
        "outputId": "79d3c901-5263-488b-a637-b5bc73d07b6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy for noise_level_10_asy: 0.7022\n",
            "Test Accuracy for noise_level_30_asy: 0.6811\n",
            "Test Accuracy for noise_level_50_asy: 0.5394\n",
            "Test Accuracy for noise_level_80_asy: 0.4545\n",
            "Test Accuracy for noise_level_90_asy: 0.4398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying dynamic weights of samples to the BaselineModel\n",
        "\n",
        "- use compute_sample_weights(), and integrate the weighted loss calculation into the training loop\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*hyper parameter tuning*\n",
        "\n",
        "1.   num of epoch\n",
        "2.   threshold in compute_sample_weights() : Starting with a low threshold and gradually increasing it to observe how the model's performance changes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "amooQLW9-GMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute sample weights based on prediction confidence\n",
        "#def compute_sample_weights(outputs, labels, threshold=0.6):\n",
        "#    confidences, _ = torch.max(nn.functional.softmax(outputs, dim=1), dim=1)\n",
        "#    weights = torch.where(confidences < threshold, torch.tensor(1.0), torch.tensor(2.0))\n",
        "#    return weights"
      ],
      "metadata": {
        "id": "_ZvOW4j7N1Sk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute sample weights based on prediction confidence with threshold\n",
        "def compute_sample_weights(outputs, labels, threshold=0.6):\n",
        "    confidences, _ = torch.max(nn.functional.softmax(outputs, dim=1), dim=1)\n",
        "    weights = torch.where(confidences < threshold, torch.tensor(1.0), 1.0 / confidences)\n",
        "    return weights"
      ],
      "metadata": {
        "id": "mIXpl25fXFiZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train & Validation and Testing - symmetric noise label"
      ],
      "metadata": {
        "id": "vojl0-OiThXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# noise_levels\n",
        "noise_levels = [0.1, 0.3, 0.5, 0.8, 0.9]\n",
        "\n",
        "# create a dictionary with keys in the format 'noise_level_{100 * value}'\n",
        "model_dict = {f'noise_level_{int(100 * level)}_sy': None for level in noise_levels}\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Training loop with dynamic sample weighting\n",
        "for epsilon in noise_levels:\n",
        "    num_epochs = 5\n",
        "\n",
        "    model = BaselineModel()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    print(f\"Symmetric Training with noise level: {epsilon}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            # Add symmetric noise to labels\n",
        "            labels = apply_label_noise(labels.numpy(), epsilon=epsilon, noise_type='symmetric')\n",
        "            labels = torch.from_numpy(labels)\n",
        "\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute sample weights using the updated function\n",
        "            weights = compute_sample_weights(outputs, labels)\n",
        "\n",
        "            # Compute weighted loss\n",
        "            loss = torch.mean(weights * criterion(outputs, labels))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Use the original CrossEntropyLoss without weighting for validation\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_accuracy = correct / total\n",
        "    average_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    print(f'Validation Loss: {average_val_loss}, Validation Accuracy: {val_accuracy}')\n",
        "\n",
        "    # Save model to dictionary\n",
        "    model_dict[f'noise_level_{int(100 * epsilon)}_sy'] = {\n",
        "        'state_dict': model.state_dict(),\n",
        "        'validation_loss': average_val_loss,\n",
        "        'validation_accuracy': val_accuracy\n",
        "    }\n"
      ],
      "metadata": {
        "id": "H4UyopAs-MDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f42c33fb-4d2d-4832-bffd-0bdd476daa16"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symmetric Training with noise level: 0.1\n",
            "Epoch 1/5, Loss: 1.7489376068115234\n",
            "Epoch 2/5, Loss: 1.6000241041183472\n",
            "Epoch 3/5, Loss: 1.5442806482315063\n",
            "Epoch 4/5, Loss: 1.458237648010254\n",
            "Epoch 5/5, Loss: 1.343650221824646\n",
            "Validation Loss: 0.9062499176165101, Validation Accuracy: 0.7026\n",
            "Symmetric Training with noise level: 0.3\n",
            "Epoch 1/5, Loss: 2.3898043632507324\n",
            "Epoch 2/5, Loss: 2.2614850997924805\n",
            "Epoch 3/5, Loss: 2.120962142944336\n",
            "Epoch 4/5, Loss: 2.144461154937744\n",
            "Epoch 5/5, Loss: 1.9440466165542603\n",
            "Validation Loss: 1.1075519858651859, Validation Accuracy: 0.6374\n",
            "Symmetric Training with noise level: 0.5\n",
            "Epoch 1/5, Loss: 2.1373746395111084\n",
            "Epoch 2/5, Loss: 2.439639091491699\n",
            "Epoch 3/5, Loss: 2.4658823013305664\n",
            "Epoch 4/5, Loss: 2.3039183616638184\n",
            "Epoch 5/5, Loss: 2.4154462814331055\n",
            "Validation Loss: 1.2760925087959143, Validation Accuracy: 0.6011\n",
            "Symmetric Training with noise level: 0.8\n",
            "Epoch 1/5, Loss: 2.2473795413970947\n",
            "Epoch 2/5, Loss: 2.276870012283325\n",
            "Epoch 3/5, Loss: 2.299630641937256\n",
            "Epoch 4/5, Loss: 2.2624590396881104\n",
            "Epoch 5/5, Loss: 2.284754991531372\n",
            "Validation Loss: 2.1000836032211403, Validation Accuracy: 0.3605\n",
            "Symmetric Training with noise level: 0.9\n",
            "Epoch 1/5, Loss: 2.3000290393829346\n",
            "Epoch 2/5, Loss: 2.3032217025756836\n",
            "Epoch 3/5, Loss: 2.3049707412719727\n",
            "Epoch 4/5, Loss: 2.301881790161133\n",
            "Epoch 5/5, Loss: 2.303619146347046\n",
            "Validation Loss: 2.303045509727138, Validation Accuracy: 0.0982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "for key, model_state_info in model_dict.items():\n",
        "\n",
        "    model = BaselineModel()\n",
        "    model_state = model.state_dict()\n",
        "    model_state.update({k: v for k, v in model_state_info['state_dict'].items() if k in model_state})\n",
        "\n",
        "    # Load the updated state_dict\n",
        "    model.load_state_dict(model_state)\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Use the original CrossEntropyLoss without weighting for testing\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Test Accuracy for {key}: {accuracy}')\n"
      ],
      "metadata": {
        "id": "cld4cbTx-MFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd2f092a-1944-409a-fc95-c3a1ac530af3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy for noise_level_10_sy: 0.7026\n",
            "Test Accuracy for noise_level_30_sy: 0.6374\n",
            "Test Accuracy for noise_level_50_sy: 0.6011\n",
            "Test Accuracy for noise_level_80_sy: 0.3605\n",
            "Test Accuracy for noise_level_90_sy: 0.0982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train & Validation and Testing - asymmetric noise label"
      ],
      "metadata": {
        "id": "AcWdTzwnTsJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# noise_levels\n",
        "noise_levels = [0.1, 0.3, 0.5, 0.8, 0.9]\n",
        "\n",
        "# create a dictionary with keys in the format 'noise_level_{100 * value}'\n",
        "model_dict = {f'noise_level_{int(100 * level)}_asy': None for level in noise_levels}\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Training loop with dynamic sample weighting\n",
        "for epsilon in noise_levels:\n",
        "    num_epochs = 5\n",
        "\n",
        "    model = BaselineModel()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    print(f\"Asymmetric Training with noise level: {epsilon}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            # Add symmetric noise to labels\n",
        "            labels = apply_label_noise(labels.numpy(), epsilon=epsilon, noise_type='asymmetric')\n",
        "            labels = torch.from_numpy(labels)\n",
        "\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute sample weights using the updated function\n",
        "            weights = compute_sample_weights(outputs, labels)\n",
        "\n",
        "            # Compute weighted loss\n",
        "            loss = torch.mean(weights * criterion(outputs, labels))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Use the original CrossEntropyLoss without weighting for validation\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_accuracy = correct / total\n",
        "    average_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    print(f'Validation Loss: {average_val_loss}, Validation Accuracy: {val_accuracy}')\n",
        "\n",
        "    # Save model to dictionary\n",
        "    model_dict[f'noise_level_{int(100 * epsilon)}_asy'] = {\n",
        "        'state_dict': model.state_dict(),\n",
        "        'validation_loss': average_val_loss,\n",
        "        'validation_accuracy': val_accuracy\n",
        "    }\n"
      ],
      "metadata": {
        "id": "6WxlTvjhTsl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "for key, model_state_info in model_dict.items():\n",
        "\n",
        "    model = BaselineModel()\n",
        "    model_state = model.state_dict()\n",
        "    model_state.update({k: v for k, v in model_state_info['state_dict'].items() if k in model_state})\n",
        "\n",
        "    # Load the updated state_dict\n",
        "    model.load_state_dict(model_state)\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Use the original CrossEntropyLoss without weighting for testing\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f'Test Accuracy for {key}: {accuracy}')\n"
      ],
      "metadata": {
        "id": "x7pZubb9Tsoy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}