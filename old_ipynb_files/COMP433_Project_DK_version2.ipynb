{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suinkangme/comp433_project/blob/main/COMP433_Project_DK_version2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Developing a robust CNN model to address the challenge of learning with label noise in  CIFAR10 dataset\n",
        "\n",
        "- CIFAR10 Label : ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’.\n",
        "\n",
        "- image size : 3x32x32\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TyD3ZYpdMfXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR"
      ],
      "metadata": {
        "id": "ItN3Ur88Mbn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training on GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "0CEWNvdcTCC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and normalize CIFAR10"
      ],
      "metadata": {
        "id": "gt98-baGNJdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the mean and standard deviation of CIFAR-10 dataset\n",
        "# cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
        "# cifar10_std = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "# imagenet..\n",
        "imagenet_mean = (0.485, 0.456, 0.406)\n",
        "imagenet_std = (0.229, 0.224, 0.225)\n",
        "\n",
        "\n",
        "# Training transformations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
        "])\n",
        "\n",
        "# Validation and testing transformations\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
        "])"
      ],
      "metadata": {
        "id": "_qbr2U9H1foB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download dataset for training\n",
        "cifar_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
        "\n",
        "# 데이터를 train과 validation으로 나누기 위해 인덱스 생성\n",
        "dataset_size = len(cifar_dataset)\n",
        "validation_split = 0.2\n",
        "val_size = int(validation_split * dataset_size)\n",
        "train_size = dataset_size - val_size\n",
        "\n",
        "# 데이터를 나누기\n",
        "train_dataset, val_dataset = random_split(cifar_dataset, [train_size, val_size])\n",
        "\n",
        "# 적절한 transform 적용\n",
        "train_dataset.dataset.transform = transform_train\n",
        "val_dataset.dataset.transform = transform_val\n",
        "\n",
        "# 데이터 로더 설정\n",
        "batch_size = 512\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umTQUZh9V1xS",
        "outputId": "e991de5b-b5f3-4c92-c82d-966eb8b86ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = cifar_dataset.classes\n",
        "print(class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPx-WcV1JSSz",
        "outputId": "bdc94052-643c-4208-d9a0-69a3e17ad48b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDBA6Zv64Lt9",
        "outputId": "f9e42afb-938a-4721-8291-7419c3b82be1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(train_dataset[4][0].permute(1, 2, 0).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "H5wOMIhl4rn_",
        "outputId": "a0ab62d4-8f94-460c-c18e-337fa3d340b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7b6d574cba90>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# test dataset\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                          train=False,\n",
        "                                          download=True,\n",
        "                                          transform = transform_test)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce3wRQHnNG9h",
        "outputId": "d9a531c5-959b-4f60-8fbf-b031ebea0809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define & train with a  baseline CNN model"
      ],
      "metadata": {
        "id": "6nGN9QRnOfLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BaselineModel,self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "      nn.Conv2d(3, 8, kernel_size=3, padding = 1),  # (input channel, output channels, kernel size, padding)  32*32*8\n",
        "      nn.ReLU(inplace=True), # activation function modifies the input tensor directly\n",
        "      nn.Conv2d(8, 16, kernel_size=3, padding=1),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=2,stride=2), # 16*16*16\n",
        "\n",
        "      nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "      nn.MaxPool2d(kernel_size=2,stride=2) # 8*8*128\n",
        "    )\n",
        "\n",
        "    # # fully connected layers\n",
        "    # self.fc_layers = nn.Sequential(\n",
        "    #   nn.Linear(128*8*8, 120),\n",
        "    #   nn.ReLU(inplace=True),\n",
        "    #   nn.Linear(120,84),\n",
        "    #   nn.ReLU(inplace=True),\n",
        "    #   nn.Linear(84,10)\n",
        "    # )\n",
        "\n",
        "    # fully connected layers\n",
        "    self.fc_layers = nn.Sequential(\n",
        "      nn.Linear(128*8*8, 512),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Linear(512,10)\n",
        "    )\n",
        "\n",
        "\n",
        "    # Apply Xavier initialization to the weights of linear layers\n",
        "    for m in self.fc_layers:\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = torch.flatten(x,1)\n",
        "    x = self.fc_layers(x)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "dOrsrTZ9M7wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = BaselineModel().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(base_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=120) # Learning rate scheduler (cosine annealing)\n"
      ],
      "metadata": {
        "id": "sIhU5JCnOx6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    base_model.train()\n",
        "    total_train_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        # Move data to GPU\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = base_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    average_train_loss = total_train_loss / len(train_loader)\n",
        "    train_acc = 100 * correct_train / total_train\n",
        "\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    base_model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            # Move data to GPU\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = base_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    average_val_loss = total_val_loss / len(val_loader)\n",
        "    val_acc = 100 * correct_val / total_val\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
        "          f'Train Loss: {average_train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, '\n",
        "          f'Validation Loss: {average_val_loss:.4f}, Validation Accuracy: {val_acc:.2f}')\n"
      ],
      "metadata": {
        "id": "rZc3RsSbO2Qt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c80e7a56-42d7-41c1-9de5-16631d018db4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 2.1701, Train Accuracy: 20.16%, Validation Loss: 2.0223, Validation Accuracy: 24.61\n",
            "Epoch 2/10, Train Loss: 1.8545, Train Accuracy: 32.88%, Validation Loss: 1.7341, Validation Accuracy: 36.91\n",
            "Epoch 3/10, Train Loss: 1.6457, Train Accuracy: 40.88%, Validation Loss: 1.5873, Validation Accuracy: 42.76\n",
            "Epoch 4/10, Train Loss: 1.5351, Train Accuracy: 44.83%, Validation Loss: 1.5002, Validation Accuracy: 45.68\n",
            "Epoch 5/10, Train Loss: 1.4446, Train Accuracy: 48.20%, Validation Loss: 1.4670, Validation Accuracy: 46.99\n",
            "Epoch 6/10, Train Loss: 1.3562, Train Accuracy: 50.92%, Validation Loss: 1.3940, Validation Accuracy: 50.29\n",
            "Epoch 7/10, Train Loss: 1.2889, Train Accuracy: 54.19%, Validation Loss: 1.2715, Validation Accuracy: 54.84\n",
            "Epoch 8/10, Train Loss: 1.2240, Train Accuracy: 56.38%, Validation Loss: 1.4044, Validation Accuracy: 50.43\n",
            "Epoch 9/10, Train Loss: 1.1780, Train Accuracy: 58.14%, Validation Loss: 1.1567, Validation Accuracy: 58.98\n",
            "Epoch 10/10, Train Loss: 1.1175, Train Accuracy: 60.42%, Validation Loss: 1.1783, Validation Accuracy: 57.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "base_model.eval()\n",
        "\n",
        "# Variables to store predictions and ground truth labels\n",
        "num_correct_predictions = 0\n",
        "total_num_predictions = 0\n",
        "test_loss = 0.0\n",
        "\n",
        "# Iterate over the test dataset\n",
        "with torch.no_grad():  # temporarily set all requires_grad flags to False\n",
        "    for inputs, labels in test_loader:\n",
        "        # Move data to GPU\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = base_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Compute loss and number of accurate predictions\n",
        "        test_loss += loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        num_correct_predictions += (preds == labels).sum().item()\n",
        "        total_num_predictions += labels.size(0)\n",
        "\n",
        "# Compute average test loss\n",
        "average_test_loss = test_loss / len(test_loader.dataset)\n",
        "\n",
        "# Compute accuracy percentage\n",
        "accuracy = (num_correct_predictions / total_num_predictions) * 100\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%, Average Test Loss: {average_test_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnhiYGcF2kG7",
        "outputId": "33a55c99-d3d1-4132-a7da-5527031e7933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 58.23%, Average Test Loss: 0.0023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Noise Labeling\n",
        "- 5 different noise levels (10%,\n",
        "30%, 50%, 80%, 90%)"
      ],
      "metadata": {
        "id": "-U2PfzQbqjq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_label_noise(labels, epsilon, noise_type):\n",
        "    num_labels = len(labels)\n",
        "    num_flips = int(epsilon * num_labels)\n",
        "\n",
        "    if noise_type == 'symmetric':\n",
        "        # Symmetric label noise\n",
        "        flip_indices = np.random.choice(num_labels, num_flips, replace=False)\n",
        "        labels[flip_indices] = np.random.randint(0, 10, num_flips)\n",
        "    elif noise_type == 'asymmetric':\n",
        "        # Asymmetric label noise\n",
        "        flip_rules = {\n",
        "            9: 1,   # Truck to Automobile\n",
        "            2: 0,   # Bird to Airplane\n",
        "            4: 7,   # Deer to Horse\n",
        "            3: 5,   # Cat to Dog\n",
        "            5: 3,   # Dog to Cat\n",
        "        }\n",
        "\n",
        "        for i in range(num_labels):\n",
        "            if np.random.random() < epsilon:\n",
        "                labels[i] = flip_rules.get(labels[i], labels[i])\n",
        "\n",
        "    return labels"
      ],
      "metadata": {
        "id": "n27NH2TooEFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Symmetric label noise"
      ],
      "metadata": {
        "id": "xBci2_LerBLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def replace_symmetric_noise(labels, epsilon):\n",
        "#     num_labels = len(labels)\n",
        "#     num_flips = int(epsilon * num_labels)\n",
        "\n",
        "#     # choose the label to be flipped\n",
        "#     flip_indices = np.random.choice(num_labels, num_flips, replace=True)\n",
        "\n",
        "#     # filp the label\n",
        "#     labels[flip_indices] = np.random.randint(0, 10, num_flips)\n",
        "\n",
        "#     return labels"
      ],
      "metadata": {
        "id": "oFdDk4NVq7bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Asymmetric label noise"
      ],
      "metadata": {
        "id": "9QPh92lorDFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def flip_labels_asymmetrically(labels, epsilon):\n",
        "#     flip_rules = {\n",
        "#         9: 1,   # Truck to Automobile\n",
        "#         2: 0,   # Bird to Airplane\n",
        "#         4: 7,   # Deer to Horse\n",
        "#         3: 5,   # Cat to Dog\n",
        "#         5: 3,   # Dog to Cat\n",
        "#     }\n",
        "\n",
        "#     flipped_labels = []\n",
        "#     for label in labels:\n",
        "#         # Check if label flipping should occur based on epsilon\n",
        "#         if np.random.random() < epsilon:\n",
        "#             # Flip the label based on the flip_rules dictionary\n",
        "#             flipped_label = flip_rules.get(label, label)\n",
        "#             flipped_labels.append(flipped_label)\n",
        "#         else:\n",
        "#             # If no flipping, keep the original label\n",
        "#             flipped_labels.append(label)\n",
        "\n",
        "#     return np.array(flipped_labels)\n"
      ],
      "metadata": {
        "id": "9JbGzoKTq7f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model with the noise labeling"
      ],
      "metadata": {
        "id": "FCwqkc3bPFcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train with the symmetric noise labeling"
      ],
      "metadata": {
        "id": "5V9Glhks0h9G"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4JTzFWLk3kpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# noise_levels\n",
        "noise_levels = [0.0, 0.2, 0.4, 0.6, 0.8]\n",
        "\n",
        "# Create a dictionary with keys in the format 'noise_level_{100 * value}'\n",
        "symmetric_model_dict = {f'noise_level_{int(100 * level)}_sy': None for level in noise_levels}\n",
        "\n",
        "for epsilon in noise_levels:\n",
        "\n",
        "    # Define different model for each nose level\n",
        "    net = BaselineModel().to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=120) # Learning rate scheduler (cosine annealing)\n",
        "\n",
        "\n",
        "    num_epochs = 7\n",
        "\n",
        "\n",
        "    # net.train()\n",
        "\n",
        "\n",
        "    print(f\"Symmetric Training with noise level: {epsilon}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        net.train()\n",
        "        total_train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "\n",
        "            labels_noisy = torch.from_numpy(apply_label_noise(labels.numpy(), epsilon=epsilon, noise_type='symmetric'))\n",
        "\n",
        "            # labels_noisy = torch.from_numpy(replace_symmetric_noise(labels.numpy(), epsilon))\n",
        "\n",
        "            # Move data to GPU\n",
        "            inputs, labels_noisy = inputs.to(device), labels_noisy.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels_noisy)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels_noisy.size(0)\n",
        "            correct_train += (predicted == labels_noisy).sum().item()\n",
        "\n",
        "        average_train_loss = total_train_loss / len(train_loader)\n",
        "        train_acc = 100 * correct_train / total_train\n",
        "\n",
        "        # Validation\n",
        "        net.eval()\n",
        "        total_val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                # Move data to GPU\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = net(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                total_val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        average_val_loss = total_val_loss / len(val_loader)\n",
        "        val_acc = 100 * correct_val / total_val\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
        "              f'Train Loss: {average_train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, '\n",
        "              f'Validation Loss: {average_val_loss:.4f}, Validation Accuracy: {val_acc:.2f}')\n",
        "\n",
        "        # save model to dictionary\n",
        "        symmetric_model_dict[f'noise_level_{int(100 * epsilon)}_sy'] = net\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGlDJFFYZrD0",
        "outputId": "f826fd38-c551-473d-ff7d-21bbb1c700b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symmetric Training with noise level: 0.0\n",
            "Epoch 1/7, Train Loss: 2.1920, Train Accuracy: 19.79%, Validation Loss: 2.0084, Validation Accuracy: 26.39\n",
            "Epoch 2/7, Train Loss: 1.8990, Train Accuracy: 30.45%, Validation Loss: 1.8096, Validation Accuracy: 34.37\n",
            "Epoch 3/7, Train Loss: 1.7180, Train Accuracy: 36.97%, Validation Loss: 1.6111, Validation Accuracy: 41.29\n",
            "Epoch 4/7, Train Loss: 1.5926, Train Accuracy: 41.87%, Validation Loss: 1.6216, Validation Accuracy: 41.88\n",
            "Epoch 5/7, Train Loss: 1.4907, Train Accuracy: 46.16%, Validation Loss: 1.4387, Validation Accuracy: 47.98\n",
            "Epoch 6/7, Train Loss: 1.4033, Train Accuracy: 49.47%, Validation Loss: 1.3571, Validation Accuracy: 50.72\n",
            "Epoch 7/7, Train Loss: 1.3055, Train Accuracy: 53.23%, Validation Loss: 1.3013, Validation Accuracy: 53.69\n",
            "Symmetric Training with noise level: 0.2\n",
            "Epoch 1/7, Train Loss: 2.2280, Train Accuracy: 17.39%, Validation Loss: 2.0088, Validation Accuracy: 26.72\n",
            "Epoch 2/7, Train Loss: 2.0418, Train Accuracy: 27.62%, Validation Loss: 1.8293, Validation Accuracy: 34.51\n",
            "Epoch 3/7, Train Loss: 1.9553, Train Accuracy: 32.43%, Validation Loss: 1.7085, Validation Accuracy: 39.90\n",
            "Epoch 4/7, Train Loss: 1.8824, Train Accuracy: 35.60%, Validation Loss: 1.6191, Validation Accuracy: 42.75\n",
            "Epoch 5/7, Train Loss: 1.8492, Train Accuracy: 37.56%, Validation Loss: 1.5533, Validation Accuracy: 46.28\n",
            "Epoch 6/7, Train Loss: 1.8030, Train Accuracy: 39.55%, Validation Loss: 1.5306, Validation Accuracy: 46.82\n",
            "Epoch 7/7, Train Loss: 1.7794, Train Accuracy: 40.62%, Validation Loss: 1.4911, Validation Accuracy: 48.66\n",
            "Symmetric Training with noise level: 0.4\n",
            "Epoch 1/7, Train Loss: 2.2779, Train Accuracy: 14.28%, Validation Loss: 2.0952, Validation Accuracy: 23.88\n",
            "Epoch 2/7, Train Loss: 2.1841, Train Accuracy: 21.48%, Validation Loss: 1.9630, Validation Accuracy: 31.48\n",
            "Epoch 3/7, Train Loss: 2.1401, Train Accuracy: 24.45%, Validation Loss: 1.8907, Validation Accuracy: 34.39\n",
            "Epoch 4/7, Train Loss: 2.1096, Train Accuracy: 26.22%, Validation Loss: 1.8228, Validation Accuracy: 37.71\n",
            "Epoch 5/7, Train Loss: 2.0793, Train Accuracy: 28.15%, Validation Loss: 1.7596, Validation Accuracy: 41.27\n",
            "Epoch 6/7, Train Loss: 2.0529, Train Accuracy: 29.76%, Validation Loss: 1.6993, Validation Accuracy: 44.08\n",
            "Epoch 7/7, Train Loss: 2.0318, Train Accuracy: 30.87%, Validation Loss: 1.6882, Validation Accuracy: 45.47\n",
            "Symmetric Training with noise level: 0.6\n",
            "Epoch 1/7, Train Loss: 2.2964, Train Accuracy: 12.19%, Validation Loss: 2.2405, Validation Accuracy: 20.92\n",
            "Epoch 2/7, Train Loss: 2.2701, Train Accuracy: 15.32%, Validation Loss: 2.1328, Validation Accuracy: 27.19\n",
            "Epoch 3/7, Train Loss: 2.2495, Train Accuracy: 17.42%, Validation Loss: 2.0831, Validation Accuracy: 28.70\n",
            "Epoch 4/7, Train Loss: 2.2395, Train Accuracy: 18.70%, Validation Loss: 2.0303, Validation Accuracy: 31.69\n",
            "Epoch 5/7, Train Loss: 2.2301, Train Accuracy: 19.43%, Validation Loss: 2.0041, Validation Accuracy: 34.70\n",
            "Epoch 6/7, Train Loss: 2.2186, Train Accuracy: 20.45%, Validation Loss: 1.9908, Validation Accuracy: 33.70\n",
            "Epoch 7/7, Train Loss: 2.2140, Train Accuracy: 20.42%, Validation Loss: 1.9447, Validation Accuracy: 38.88\n",
            "Symmetric Training with noise level: 0.8\n",
            "Epoch 1/7, Train Loss: 2.3017, Train Accuracy: 10.92%, Validation Loss: 2.2842, Validation Accuracy: 17.20\n",
            "Epoch 2/7, Train Loss: 2.2979, Train Accuracy: 11.86%, Validation Loss: 2.2404, Validation Accuracy: 22.99\n",
            "Epoch 3/7, Train Loss: 2.2944, Train Accuracy: 12.29%, Validation Loss: 2.2170, Validation Accuracy: 22.25\n",
            "Epoch 4/7, Train Loss: 2.2901, Train Accuracy: 13.20%, Validation Loss: 2.1936, Validation Accuracy: 23.74\n",
            "Epoch 5/7, Train Loss: 2.2903, Train Accuracy: 13.32%, Validation Loss: 2.1987, Validation Accuracy: 27.14\n",
            "Epoch 6/7, Train Loss: 2.2904, Train Accuracy: 13.67%, Validation Loss: 2.1771, Validation Accuracy: 29.91\n",
            "Epoch 7/7, Train Loss: 2.2883, Train Accuracy: 13.85%, Validation Loss: 2.1834, Validation Accuracy: 30.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "5yXcz4Ye8fdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key, net in symmetric_model_dict.items():\n",
        "    net.eval()\n",
        "    # net.to(device)\n",
        "\n",
        "    # Variables to store predictions and ground truth labels\n",
        "    num_correct_predictions = 0\n",
        "    total_num_predictions = 0\n",
        "    loss = 0.0\n",
        "\n",
        "    # Iterate over the test dataset\n",
        "    with torch.no_grad():  # temporarily set all requires_grad flags to False\n",
        "        for i, (data, label) in enumerate(test_loader):\n",
        "            # move inputs to desired device and dtype\n",
        "            data = data.to(device, dtype=torch.float32)\n",
        "            label = label.to(device, dtype=torch.long)\n",
        "\n",
        "            # forward pass\n",
        "            logit = net(data)\n",
        "\n",
        "            # compute loss and number of accurate predictions\n",
        "            loss += torch.nn.functional.cross_entropy(logit, label, reduction='sum').item()\n",
        "            preds = logit.max(dim=1)[1]\n",
        "            num_correct_predictions += (preds == label).sum().item()\n",
        "            total_num_predictions += len(preds)\n",
        "\n",
        "        # compute average loss\n",
        "        loss /= total_num_predictions\n",
        "\n",
        "        # compute accuracy percentage\n",
        "        accuracy = (float(num_correct_predictions) / total_num_predictions) * 100\n",
        "\n",
        "        print(f\"Symmetric {key} Accuracy: {accuracy:.2f}%, Loss: {loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "z0ax5iBOPY_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cc3f4ac-644b-4061-a815-e28831377ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symmetric noise_level_0_sy Accuracy: 53.41%, Loss: 1.2921\n",
            "Symmetric noise_level_20_sy Accuracy: 49.82%, Loss: 1.4822\n",
            "Symmetric noise_level_40_sy Accuracy: 46.57%, Loss: 1.6782\n",
            "Symmetric noise_level_60_sy Accuracy: 38.51%, Loss: 1.9427\n",
            "Symmetric noise_level_80_sy Accuracy: 31.13%, Loss: 2.1817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train with the asymmetric noise labeling"
      ],
      "metadata": {
        "id": "8N3-1RaLih-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# noise_levels\n",
        "noise_levels = [0.0, 0.1, 0.2, 0.3, 0.4]\n",
        "# Create a dictionary with keys in the format 'noise_level_{100 * value}'\n",
        "asymmetric_model_dict = {f'noise_level_{int(100 * level)}_asy': None for level in noise_levels}\n",
        "\n",
        "\n",
        "\n",
        "# # Training on GPU\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "for epsilon in noise_levels:\n",
        "\n",
        "\n",
        "    net = BaselineModel().to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=120) # Learning rate scheduler (cosine annealing)\n",
        "\n",
        "    num_epochs = 5\n",
        "\n",
        "\n",
        "    # net.train()\n",
        "\n",
        "\n",
        "    print(f\"Asymmetric Training with noise level: {epsilon}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      # Training\n",
        "      net.train()\n",
        "      total_train_loss = 0.0\n",
        "      correct_train = 0\n",
        "      total_train = 0\n",
        "\n",
        "      for inputs, labels in train_loader:\n",
        "          labels_noisy = torch.from_numpy(apply_label_noise(labels.numpy(), epsilon=epsilon, noise_type='asymmetric'))\n",
        "\n",
        "          # Move data to GPU\n",
        "          inputs, labels_noisy = inputs.to(device), labels_noisy.to(device)\n",
        "\n",
        "          # Zero the gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = net(inputs)\n",
        "          loss = criterion(outputs, labels_noisy)\n",
        "\n",
        "          # Backward pass and optimization\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          total_train_loss += loss.item()\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total_train += labels_noisy.size(0)\n",
        "          correct_train += (predicted == labels_noisy).sum().item()\n",
        "\n",
        "      average_train_loss = total_train_loss / len(train_loader)\n",
        "      train_acc = 100 * correct_train / total_train\n",
        "\n",
        "\n",
        "      scheduler.step()\n",
        "\n",
        "      # Validation\n",
        "      net.eval()\n",
        "      total_val_loss = 0.0\n",
        "      correct_val = 0\n",
        "      total_val = 0\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for inputs, labels in val_loader:\n",
        "              # Move data to GPU\n",
        "              inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "              # Forward pass\n",
        "              outputs = net(inputs)\n",
        "              loss = criterion(outputs, labels)\n",
        "\n",
        "              total_val_loss += loss.item()\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              total_val += labels.size(0)\n",
        "              correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "      average_val_loss = total_val_loss / len(val_loader)\n",
        "      val_acc = 100 * correct_val / total_val\n",
        "\n",
        "      print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
        "            f'Train Loss: {average_train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, '\n",
        "            f'Validation Loss: {average_val_loss:.4f}, Validation Accuracy: {val_acc:.2f}')\n",
        "      # save model to dictionary\n",
        "      asymmetric_model_dict[f'noise_level_{int(100 * epsilon)}_asy'] = net\n"
      ],
      "metadata": {
        "id": "zRiBGV6bPf6n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f5cd74c-1e5c-4c91-91f9-fe89f47730ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Asymmetric Training with noise level: 0.0\n",
            "Epoch 1/5, Train Loss: 2.1758, Train Accuracy: 19.13%, Validation Loss: 2.0879, Validation Accuracy: 23.86\n",
            "Epoch 2/5, Train Loss: 1.9224, Train Accuracy: 29.55%, Validation Loss: 1.8606, Validation Accuracy: 33.65\n",
            "Epoch 3/5, Train Loss: 1.7143, Train Accuracy: 37.97%, Validation Loss: 1.6070, Validation Accuracy: 41.08\n",
            "Epoch 4/5, Train Loss: 1.5741, Train Accuracy: 43.37%, Validation Loss: 1.5294, Validation Accuracy: 44.76\n",
            "Epoch 5/5, Train Loss: 1.4918, Train Accuracy: 46.23%, Validation Loss: 1.4519, Validation Accuracy: 47.53\n",
            "Asymmetric Training with noise level: 0.1\n",
            "Epoch 1/5, Train Loss: 2.2038, Train Accuracy: 17.28%, Validation Loss: 2.0265, Validation Accuracy: 25.86\n",
            "Epoch 2/5, Train Loss: 1.8969, Train Accuracy: 31.32%, Validation Loss: 1.8007, Validation Accuracy: 34.84\n",
            "Epoch 3/5, Train Loss: 1.6966, Train Accuracy: 39.13%, Validation Loss: 1.5910, Validation Accuracy: 41.65\n",
            "Epoch 4/5, Train Loss: 1.5889, Train Accuracy: 42.98%, Validation Loss: 1.5116, Validation Accuracy: 45.36\n",
            "Epoch 5/5, Train Loss: 1.5063, Train Accuracy: 45.97%, Validation Loss: 1.4694, Validation Accuracy: 47.17\n",
            "Asymmetric Training with noise level: 0.2\n",
            "Epoch 1/5, Train Loss: 2.1504, Train Accuracy: 21.32%, Validation Loss: 1.9675, Validation Accuracy: 26.52\n",
            "Epoch 2/5, Train Loss: 1.8452, Train Accuracy: 33.28%, Validation Loss: 1.7704, Validation Accuracy: 34.51\n",
            "Epoch 3/5, Train Loss: 1.7053, Train Accuracy: 38.14%, Validation Loss: 1.6708, Validation Accuracy: 39.19\n",
            "Epoch 4/5, Train Loss: 1.5953, Train Accuracy: 42.06%, Validation Loss: 1.4949, Validation Accuracy: 46.07\n",
            "Epoch 5/5, Train Loss: 1.4896, Train Accuracy: 46.27%, Validation Loss: 1.5030, Validation Accuracy: 45.04\n",
            "Asymmetric Training with noise level: 0.3\n",
            "Epoch 1/5, Train Loss: 2.2056, Train Accuracy: 19.39%, Validation Loss: 2.0676, Validation Accuracy: 22.26\n",
            "Epoch 2/5, Train Loss: 1.9070, Train Accuracy: 31.01%, Validation Loss: 1.8433, Validation Accuracy: 32.56\n",
            "Epoch 3/5, Train Loss: 1.7312, Train Accuracy: 37.23%, Validation Loss: 1.6341, Validation Accuracy: 39.86\n",
            "Epoch 4/5, Train Loss: 1.6267, Train Accuracy: 41.08%, Validation Loss: 1.5687, Validation Accuracy: 43.22\n",
            "Epoch 5/5, Train Loss: 1.5498, Train Accuracy: 43.74%, Validation Loss: 1.5142, Validation Accuracy: 44.92\n",
            "Asymmetric Training with noise level: 0.4\n",
            "Epoch 1/5, Train Loss: 2.1437, Train Accuracy: 20.95%, Validation Loss: 2.0942, Validation Accuracy: 22.70\n",
            "Epoch 2/5, Train Loss: 1.8855, Train Accuracy: 32.06%, Validation Loss: 1.8576, Validation Accuracy: 31.46\n",
            "Epoch 3/5, Train Loss: 1.7138, Train Accuracy: 38.29%, Validation Loss: 1.6733, Validation Accuracy: 38.11\n",
            "Epoch 4/5, Train Loss: 1.6151, Train Accuracy: 41.38%, Validation Loss: 1.5960, Validation Accuracy: 40.49\n",
            "Epoch 5/5, Train Loss: 1.5240, Train Accuracy: 44.09%, Validation Loss: 1.5574, Validation Accuracy: 42.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "FD4NBMyA8X4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key, net in asymmetric_model_dict.items():\n",
        "    net.eval()\n",
        "    # net.to(device)\n",
        "\n",
        "    # Variables to store predictions and ground truth labels\n",
        "    num_correct_predictions = 0\n",
        "    total_num_predictions = 0\n",
        "    loss = 0.0\n",
        "\n",
        "    # Iterate over the test dataset\n",
        "    with torch.no_grad():  # temporarily set all requires_grad flags to False\n",
        "        for i, (data, label) in enumerate(test_loader):\n",
        "            # move inputs to desired device and dtype\n",
        "            data = data.to(device, dtype=torch.float32)\n",
        "            label = label.to(device, dtype=torch.long)\n",
        "\n",
        "            # forward pass\n",
        "            logit = net(data)\n",
        "\n",
        "            # compute loss and number of accurate predictions\n",
        "            loss += torch.nn.functional.cross_entropy(logit, label, reduction='sum').item()\n",
        "            preds = logit.max(dim=1)[1]\n",
        "            num_correct_predictions += (preds == label).sum().item()\n",
        "            total_num_predictions += len(preds)\n",
        "\n",
        "        # compute average loss\n",
        "        loss /= total_num_predictions\n",
        "\n",
        "        # compute accuracy percentage\n",
        "        accuracy = (float(num_correct_predictions) / total_num_predictions) * 100\n",
        "\n",
        "        print(f\"Asymmetric {key} Accuracy: {accuracy:.2f}%, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "id": "PkWK7f63Pf8w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5964a193-ea01-48fc-dda6-dffeafcb2698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Asymmetric noise_level_0_asy Accuracy: 48.39%, Loss: 1.4492\n",
            "Asymmetric noise_level_10_asy Accuracy: 47.15%, Loss: 1.4664\n",
            "Asymmetric noise_level_20_asy Accuracy: 45.26%, Loss: 1.5059\n",
            "Asymmetric noise_level_30_asy Accuracy: 44.82%, Loss: 1.5025\n",
            "Asymmetric noise_level_40_asy Accuracy: 42.50%, Loss: 1.5555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Active Passive Loss (APL)"
      ],
      "metadata": {
        "id": "dpkHZMy3iS6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Active Loss Functions**:\n",
        "<br> Cross Entropy (CE)\n",
        "<br> Normalized Cross Entropy (NCE)\n",
        "<br> Focal Loss (FL)\n",
        "<br> Normalized Focal Loss (NFL)\n",
        "<br>\n",
        "\n",
        "\n",
        "<br> **2. Passive Loss Functions**:\n",
        "<br> Mean Absolute Error (MAE)\n",
        "<br> Normalized Mean Absolute Error (NMAE)\n",
        "<br> Reverse Cross Entropy (RCE)\n",
        "<br> Normalized Reverse Cross Entropy (NRCE)\n",
        "<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "PWwvdFdOpL-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class APLLoss(nn.Module):\n",
        "    def __init__(self, alpha, beta, active_func_name , passive_func_name):\n",
        "        super(APLLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        # Get functions by name\n",
        "        self.active_loss = getattr(self, active_func_name)\n",
        "        self.passive_loss = getattr(self, passive_func_name)\n",
        "\n",
        "    # normalized_relative_cross_entropy\n",
        "    def nrce(self, outputs, targets, epsilon=1e-8):\n",
        "        pred_prob = F.softmax(outputs, dim=1)\n",
        "        true_prob = F.one_hot(targets, num_classes=outputs.size(1)).float()\n",
        "        cross_entropy = -torch.sum(true_prob * torch.log(pred_prob + epsilon), dim=1)\n",
        "        nrce = torch.mean(cross_entropy)\n",
        "        return nrce\n",
        "\n",
        "    # normalized_cross_entropy\n",
        "    def nce(self, outputs, targets, epsilon=1e-8):\n",
        "        pred_prob = F.softmax(outputs, dim=1)\n",
        "        true_prob = F.one_hot(targets, num_classes=outputs.size(1)).float()\n",
        "        cross_entropy = -torch.sum(true_prob * torch.log(pred_prob + epsilon), dim=1)\n",
        "        entropy = -torch.sum(pred_prob * torch.log(pred_prob + epsilon), dim=1)\n",
        "        nce = torch.mean(cross_entropy / entropy)\n",
        "        return nce\n",
        "\n",
        "    # relative_cross_entropy\n",
        "    def rce(self, outputs, targets, epsilon=1e-8):\n",
        "        pred_prob = F.softmax(outputs, dim=1)\n",
        "        true_prob = F.one_hot(targets, num_classes=outputs.size(1)).float()\n",
        "        cross_entropy = -torch.sum(true_prob * torch.log(pred_prob + epsilon), dim=1)\n",
        "        return torch.mean(cross_entropy)\n",
        "    # mean_absolute_error\n",
        "    def mae(self,outputs, targets):\n",
        "        true_prob = F.one_hot(targets, num_classes=outputs.size(1)).float()\n",
        "        mae = F.l1_loss(outputs, true_prob)\n",
        "        return mae\n",
        "\n",
        "    # focal_loss\n",
        "    def fl(self, outputs, targets):\n",
        "        gamma = 0.5\n",
        "        log_prob = F.log_softmax(outputs, dim=1)\n",
        "        prob = torch.exp(log_prob)\n",
        "        focal_loss = -torch.sum((1 - prob) ** gamma * log_prob * F.one_hot(targets, num_classes=outputs.size(1)).float(), dim=1)\n",
        "        return torch.mean(focal_loss)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        active_loss =  self.active_loss(outputs, targets)\n",
        "        passive_loss = self.passive_loss(outputs, targets)\n",
        "\n",
        "        apl_loss = (self.alpha * active_loss) + (self.beta * passive_loss)\n",
        "        return apl_loss\n"
      ],
      "metadata": {
        "id": "Nw9CZ_AQiSrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## APL(nce, rce) + Sym[0.0, 0.2, 0.4, 0.6, 0.8]"
      ],
      "metadata": {
        "id": "36KNyoqZFO2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 1  # 이 값은 조절 가능\n",
        "beta = 1   # 이 값은 조절 가능\n",
        "apl_criterion = APLLoss(alpha, beta,'nce','rce')\n",
        "\n",
        "\n",
        "# noise_levels\n",
        "noise_levels = [0.0, 0.2, 0.4, 0.6, 0.8]\n",
        "\n",
        "# Create a dictionary with keys in the format 'noise_level_{100 * value}'\n",
        "symmetric_model_dict = {f'noise_level_{int(100 * level)}_sy': None for level in noise_levels}\n",
        "\n",
        "print(\"Training with APL(nce,rce)\")\n",
        "\n",
        "# # Training on GPU\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "for epsilon in noise_levels:\n",
        "\n",
        "\n",
        "    # Define model for each noise level..\n",
        "    APL_model = BaselineModel().to(device)\n",
        "    optimizer = optim.SGD(APL_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=120) # Learning rate scheduler (cosine annealing)\n",
        "\n",
        "\n",
        "    num_epochs = 7\n",
        "\n",
        "    # net.train()\n",
        "\n",
        "\n",
        "    print(f\"Symmetric Training with noise level: {epsilon}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        APL_model.train()\n",
        "        total_train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            labels_noisy = torch.from_numpy(apply_label_noise(labels.numpy(), epsilon=epsilon, noise_type='symmetric'))\n",
        "\n",
        "            # Move data to GPU\n",
        "            inputs, labels_noisy = inputs.to(device), labels_noisy.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = APL_model(inputs)\n",
        "            loss = apl_criterion(outputs, labels_noisy)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels_noisy.size(0)\n",
        "            correct_train += (predicted == labels_noisy).sum().item()\n",
        "\n",
        "        average_train_loss = total_train_loss / len(train_loader)\n",
        "        train_acc = 100 * correct_train / total_train\n",
        "\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation\n",
        "        APL_model.eval()\n",
        "        total_val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                # Move data to GPU\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = APL_model(inputs)\n",
        "                loss = apl_criterion(outputs, labels)\n",
        "\n",
        "                total_val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        average_val_loss = total_val_loss / len(val_loader)\n",
        "        val_acc = 100 * correct_val / total_val\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
        "              f'Train Loss: {average_train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, '\n",
        "              f'Validation Loss: {average_val_loss:.4f}, Validation Accuracy: {val_acc:.2f}')\n",
        "        # save model to dictionary\n",
        "        symmetric_model_dict[f'noise_level_{int(100 * epsilon)}_sy'] = APL_model\n"
      ],
      "metadata": {
        "id": "BEyp2RiEMzgQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "563ec230-7ebd-49d0-cdc6-ccbb4b75a74e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with APL(nce,rce)\n",
            "Symmetric Training with noise level: 0.0\n",
            "Epoch 1/7, Train Loss: 3.0926, Train Accuracy: 22.71%, Validation Loss: 2.8593, Validation Accuracy: 32.26\n",
            "Epoch 2/7, Train Loss: 2.6977, Train Accuracy: 36.86%, Validation Loss: 2.5653, Validation Accuracy: 40.49\n",
            "Epoch 3/7, Train Loss: 2.4715, Train Accuracy: 43.67%, Validation Loss: 2.3683, Validation Accuracy: 46.88\n",
            "Epoch 4/7, Train Loss: 2.3423, Train Accuracy: 47.55%, Validation Loss: 2.2813, Validation Accuracy: 49.60\n",
            "Epoch 5/7, Train Loss: 2.2159, Train Accuracy: 51.48%, Validation Loss: 2.4184, Validation Accuracy: 47.19\n",
            "Epoch 6/7, Train Loss: 2.1505, Train Accuracy: 53.91%, Validation Loss: 2.1390, Validation Accuracy: 54.43\n",
            "Epoch 7/7, Train Loss: 2.0395, Train Accuracy: 57.53%, Validation Loss: 2.1123, Validation Accuracy: 55.45\n",
            "Symmetric Training with noise level: 0.2\n",
            "Epoch 1/7, Train Loss: 3.2337, Train Accuracy: 17.07%, Validation Loss: 2.9756, Validation Accuracy: 27.14\n",
            "Epoch 2/7, Train Loss: 2.9905, Train Accuracy: 27.97%, Validation Loss: 2.7461, Validation Accuracy: 35.68\n",
            "Epoch 3/7, Train Loss: 2.8727, Train Accuracy: 33.38%, Validation Loss: 2.5809, Validation Accuracy: 40.99\n",
            "Epoch 4/7, Train Loss: 2.7991, Train Accuracy: 36.97%, Validation Loss: 2.5102, Validation Accuracy: 45.95\n",
            "Epoch 5/7, Train Loss: 2.7440, Train Accuracy: 38.93%, Validation Loss: 2.4092, Validation Accuracy: 46.10\n",
            "Epoch 6/7, Train Loss: 2.7002, Train Accuracy: 41.22%, Validation Loss: 2.3368, Validation Accuracy: 50.57\n",
            "Epoch 7/7, Train Loss: 2.6524, Train Accuracy: 43.05%, Validation Loss: 2.2963, Validation Accuracy: 50.15\n",
            "Symmetric Training with noise level: 0.4\n",
            "Epoch 1/7, Train Loss: 3.2688, Train Accuracy: 14.44%, Validation Loss: 3.1188, Validation Accuracy: 24.05\n",
            "Epoch 2/7, Train Loss: 3.1776, Train Accuracy: 21.76%, Validation Loss: 2.9100, Validation Accuracy: 32.10\n",
            "Epoch 3/7, Train Loss: 3.1095, Train Accuracy: 25.26%, Validation Loss: 2.7994, Validation Accuracy: 36.78\n",
            "Epoch 4/7, Train Loss: 3.0756, Train Accuracy: 27.20%, Validation Loss: 2.7359, Validation Accuracy: 40.12\n",
            "Epoch 5/7, Train Loss: 3.0309, Train Accuracy: 29.36%, Validation Loss: 2.6873, Validation Accuracy: 40.66\n",
            "Epoch 6/7, Train Loss: 3.0267, Train Accuracy: 29.79%, Validation Loss: 2.6500, Validation Accuracy: 42.77\n",
            "Epoch 7/7, Train Loss: 3.0005, Train Accuracy: 31.11%, Validation Loss: 2.5698, Validation Accuracy: 45.49\n",
            "Symmetric Training with noise level: 0.6\n",
            "Epoch 1/7, Train Loss: 3.2994, Train Accuracy: 11.46%, Validation Loss: 3.2656, Validation Accuracy: 21.34\n",
            "Epoch 2/7, Train Loss: 3.2686, Train Accuracy: 15.28%, Validation Loss: 3.0955, Validation Accuracy: 29.76\n",
            "Epoch 3/7, Train Loss: 3.2341, Train Accuracy: 18.67%, Validation Loss: 3.0084, Validation Accuracy: 33.79\n",
            "Epoch 4/7, Train Loss: 3.2150, Train Accuracy: 20.12%, Validation Loss: 2.9526, Validation Accuracy: 36.19\n",
            "Epoch 5/7, Train Loss: 3.1979, Train Accuracy: 21.32%, Validation Loss: 2.9054, Validation Accuracy: 39.30\n",
            "Epoch 6/7, Train Loss: 3.1937, Train Accuracy: 22.00%, Validation Loss: 2.8729, Validation Accuracy: 40.27\n",
            "Epoch 7/7, Train Loss: 3.1821, Train Accuracy: 22.50%, Validation Loss: 2.8054, Validation Accuracy: 41.86\n",
            "Symmetric Training with noise level: 0.8\n",
            "Epoch 1/7, Train Loss: 3.3025, Train Accuracy: 10.47%, Validation Loss: 3.2940, Validation Accuracy: 12.79\n",
            "Epoch 2/7, Train Loss: 3.2997, Train Accuracy: 11.58%, Validation Loss: 3.2581, Validation Accuracy: 21.26\n",
            "Epoch 3/7, Train Loss: 3.2949, Train Accuracy: 12.46%, Validation Loss: 3.2211, Validation Accuracy: 24.86\n",
            "Epoch 4/7, Train Loss: 3.2916, Train Accuracy: 13.27%, Validation Loss: 3.1857, Validation Accuracy: 27.85\n",
            "Epoch 5/7, Train Loss: 3.2883, Train Accuracy: 13.76%, Validation Loss: 3.1520, Validation Accuracy: 28.65\n",
            "Epoch 6/7, Train Loss: 3.2862, Train Accuracy: 13.92%, Validation Loss: 3.1373, Validation Accuracy: 29.88\n",
            "Epoch 7/7, Train Loss: 3.2838, Train Accuracy: 14.41%, Validation Loss: 3.1169, Validation Accuracy: 32.57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, net in symmetric_model_dict.items():\n",
        "    net.eval()\n",
        "    # net.to(device)\n",
        "\n",
        "    # Variables to store predictions and ground truth labels\n",
        "    num_correct_predictions = 0\n",
        "    total_num_predictions = 0\n",
        "    loss = 0.0\n",
        "\n",
        "    # Iterate over the test dataset\n",
        "    with torch.no_grad():  # temporarily set all requires_grad flags to False\n",
        "        for i, (data, label) in enumerate(test_loader):\n",
        "            # move inputs to desired device and dtype\n",
        "            data = data.to(device, dtype=torch.float32)\n",
        "            label = label.to(device, dtype=torch.long)\n",
        "\n",
        "            # forward pass\n",
        "            logit = net(data)\n",
        "\n",
        "            # compute loss and number of accurate predictions\n",
        "            loss += torch.nn.functional.cross_entropy(logit, label, reduction='sum').item()\n",
        "            preds = logit.max(dim=1)[1]\n",
        "            num_correct_predictions += (preds == label).sum().item()\n",
        "            total_num_predictions += len(preds)\n",
        "\n",
        "        # compute average loss\n",
        "        loss /= total_num_predictions\n",
        "\n",
        "        # compute accuracy percentage\n",
        "        accuracy = (float(num_correct_predictions) / total_num_predictions) * 100\n",
        "\n",
        "        print(f\"Symmetric {key} Accuracy: {accuracy:.2f}%, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijAHlPICFkyJ",
        "outputId": "3784158e-d5cd-4913-9a5c-dcf27bbc4fd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symmetric noise_level_0_sy Accuracy: 55.81%, Loss: 1.2840\n",
            "Symmetric noise_level_20_sy Accuracy: 50.36%, Loss: 1.5039\n",
            "Symmetric noise_level_40_sy Accuracy: 46.60%, Loss: 1.7457\n",
            "Symmetric noise_level_60_sy Accuracy: 42.13%, Loss: 1.9364\n",
            "Symmetric noise_level_80_sy Accuracy: 33.71%, Loss: 2.1689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## APL(nce, rce) + asym[0.0, 0.1, 0.2, 0.3, 0.4]"
      ],
      "metadata": {
        "id": "xAUOXEUBFIVw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JPZnGoTfFIJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 1  # 이 값은 조절 가능\n",
        "beta = 1   # 이 값은 조절 가능\n",
        "apl_criterion = APLLoss(alpha, beta,'nce','rce')\n",
        "\n",
        "\n",
        "# noise_levels\n",
        "noise_levels = [0.0, 0.1, 0.2, 0.3, 0.4]\n",
        "# Create a dictionary with keys in the format 'noise_level_{100 * value}'\n",
        "asymmetric_model_dict = {f'noise_level_{int(100 * level)}_asy': None for level in noise_levels}\n",
        "\n",
        "print(\"Training with APL(nce,rce)\")\n",
        "\n",
        "# # Training on GPU\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "for epsilon in noise_levels:\n",
        "\n",
        "\n",
        "    # Define model for each noise level..\n",
        "    APL_model = BaselineModel().to(device)\n",
        "    optimizer = optim.SGD(APL_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=120) # Learning rate scheduler (cosine annealing)\n",
        "\n",
        "\n",
        "    num_epochs = 5\n",
        "\n",
        "    # net.train()\n",
        "\n",
        "\n",
        "    print(f\"Asymmetric Training with noise level: {epsilon}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      # Training\n",
        "      APL_model.train()\n",
        "      total_train_loss = 0.0\n",
        "      correct_train = 0\n",
        "      total_train = 0\n",
        "\n",
        "      for inputs, labels in train_loader:\n",
        "          labels_noisy = torch.from_numpy(apply_label_noise(labels.numpy(), epsilon=epsilon, noise_type='asymmetric'))\n",
        "\n",
        "          # Move data to GPU\n",
        "          inputs, labels_noisy = inputs.to(device), labels_noisy.to(device)\n",
        "\n",
        "          # Zero the gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = APL_model(inputs)\n",
        "          loss = apl_criterion(outputs, labels_noisy)\n",
        "\n",
        "          # Backward pass and optimization\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          total_train_loss += loss.item()\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total_train += labels_noisy.size(0)\n",
        "          correct_train += (predicted == labels_noisy).sum().item()\n",
        "\n",
        "      average_train_loss = total_train_loss / len(train_loader)\n",
        "      train_acc = 100 * correct_train / total_train\n",
        "\n",
        "\n",
        "      scheduler.step()\n",
        "\n",
        "      # Validation\n",
        "      APL_model.eval()\n",
        "      total_val_loss = 0.0\n",
        "      correct_val = 0\n",
        "      total_val = 0\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for inputs, labels in val_loader:\n",
        "              # Move data to GPU\n",
        "              inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "              # Forward pass\n",
        "              outputs = APL_model(inputs)\n",
        "              loss = apl_criterion(outputs, labels)\n",
        "\n",
        "              total_val_loss += loss.item()\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              total_val += labels.size(0)\n",
        "              correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "      average_val_loss = total_val_loss / len(val_loader)\n",
        "      val_acc = 100 * correct_val / total_val\n",
        "\n",
        "      print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
        "            f'Train Loss: {average_train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, '\n",
        "            f'Validation Loss: {average_val_loss:.4f}, Validation Accuracy: {val_acc:.2f}')\n",
        "      # save model to dictionary\n",
        "      asymmetric_model_dict[f'noise_level_{int(100 * epsilon)}_asy'] = APL_model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muEIA1j_TJoM",
        "outputId": "546a642e-28a7-4c0a-8110-704b6ddb3ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with APL(nce,rce)\n",
            "Asymmetric Training with noise level: 0.0\n",
            "Epoch 1/5, Train Loss: 3.0932, Train Accuracy: 22.32%, Validation Loss: 2.9200, Validation Accuracy: 28.60\n",
            "Epoch 2/5, Train Loss: 2.7054, Train Accuracy: 35.87%, Validation Loss: 2.5760, Validation Accuracy: 39.92\n",
            "Epoch 3/5, Train Loss: 2.5015, Train Accuracy: 42.51%, Validation Loss: 2.4118, Validation Accuracy: 45.69\n",
            "Epoch 4/5, Train Loss: 2.3607, Train Accuracy: 47.45%, Validation Loss: 2.3834, Validation Accuracy: 46.54\n",
            "Epoch 5/5, Train Loss: 2.2551, Train Accuracy: 50.26%, Validation Loss: 2.1996, Validation Accuracy: 52.05\n",
            "Asymmetric Training with noise level: 0.1\n",
            "Epoch 1/5, Train Loss: 3.0799, Train Accuracy: 22.67%, Validation Loss: 2.8696, Validation Accuracy: 31.83\n",
            "Epoch 2/5, Train Loss: 2.6933, Train Accuracy: 36.90%, Validation Loss: 2.6167, Validation Accuracy: 40.05\n",
            "Epoch 3/5, Train Loss: 2.5093, Train Accuracy: 42.59%, Validation Loss: 2.4841, Validation Accuracy: 43.83\n",
            "Epoch 4/5, Train Loss: 2.4037, Train Accuracy: 45.49%, Validation Loss: 2.3754, Validation Accuracy: 46.60\n",
            "Epoch 5/5, Train Loss: 2.2908, Train Accuracy: 49.35%, Validation Loss: 2.2445, Validation Accuracy: 50.71\n",
            "Asymmetric Training with noise level: 0.2\n",
            "Epoch 1/5, Train Loss: 3.0962, Train Accuracy: 22.73%, Validation Loss: 2.9608, Validation Accuracy: 26.15\n",
            "Epoch 2/5, Train Loss: 2.7581, Train Accuracy: 34.52%, Validation Loss: 2.7010, Validation Accuracy: 34.98\n",
            "Epoch 3/5, Train Loss: 2.5741, Train Accuracy: 39.96%, Validation Loss: 2.5274, Validation Accuracy: 41.20\n",
            "Epoch 4/5, Train Loss: 2.4372, Train Accuracy: 44.03%, Validation Loss: 2.3651, Validation Accuracy: 46.60\n",
            "Epoch 5/5, Train Loss: 2.3205, Train Accuracy: 48.02%, Validation Loss: 2.2532, Validation Accuracy: 50.11\n",
            "Asymmetric Training with noise level: 0.3\n",
            "Epoch 1/5, Train Loss: 3.0958, Train Accuracy: 22.33%, Validation Loss: 3.0158, Validation Accuracy: 25.42\n",
            "Epoch 2/5, Train Loss: 2.8141, Train Accuracy: 32.53%, Validation Loss: 2.7882, Validation Accuracy: 33.46\n",
            "Epoch 3/5, Train Loss: 2.5757, Train Accuracy: 40.24%, Validation Loss: 2.4799, Validation Accuracy: 42.60\n",
            "Epoch 4/5, Train Loss: 2.4279, Train Accuracy: 44.61%, Validation Loss: 2.3685, Validation Accuracy: 46.51\n",
            "Epoch 5/5, Train Loss: 2.3325, Train Accuracy: 47.51%, Validation Loss: 2.2931, Validation Accuracy: 48.42\n",
            "Asymmetric Training with noise level: 0.4\n",
            "Epoch 1/5, Train Loss: 3.1095, Train Accuracy: 22.31%, Validation Loss: 2.9611, Validation Accuracy: 26.74\n",
            "Epoch 2/5, Train Loss: 2.7398, Train Accuracy: 34.95%, Validation Loss: 2.7329, Validation Accuracy: 34.00\n",
            "Epoch 3/5, Train Loss: 2.5677, Train Accuracy: 40.21%, Validation Loss: 2.5406, Validation Accuracy: 40.46\n",
            "Epoch 4/5, Train Loss: 2.4351, Train Accuracy: 43.97%, Validation Loss: 2.4516, Validation Accuracy: 43.03\n",
            "Epoch 5/5, Train Loss: 2.3581, Train Accuracy: 46.08%, Validation Loss: 2.4128, Validation Accuracy: 42.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "1ckW2OXWFVe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key, net in asymmetric_model_dict.items():\n",
        "    net.eval()\n",
        "    # net.to(device)\n",
        "\n",
        "    # Variables to store predictions and ground truth labels\n",
        "    num_correct_predictions = 0\n",
        "    total_num_predictions = 0\n",
        "    loss = 0.0\n",
        "\n",
        "    # Iterate over the test dataset\n",
        "    with torch.no_grad():  # temporarily set all requires_grad flags to False\n",
        "        for i, (data, label) in enumerate(test_loader):\n",
        "            # move inputs to desired device and dtype\n",
        "            data = data.to(device, dtype=torch.float32)\n",
        "            label = label.to(device, dtype=torch.long)\n",
        "\n",
        "            # forward pass\n",
        "            logit = net(data)\n",
        "\n",
        "            # compute loss and number of accurate predictions\n",
        "            loss += torch.nn.functional.cross_entropy(logit, label, reduction='sum').item()\n",
        "            preds = logit.max(dim=1)[1]\n",
        "            num_correct_predictions += (preds == label).sum().item()\n",
        "            total_num_predictions += len(preds)\n",
        "\n",
        "        # compute average loss\n",
        "        loss /= total_num_predictions\n",
        "\n",
        "        # compute accuracy percentage\n",
        "        accuracy = (float(num_correct_predictions) / total_num_predictions) * 100\n",
        "\n",
        "        print(f\"Asymmetric {key} Accuracy: {accuracy:.2f}%, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "id": "Zz_Aom6Rxifz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77fca052-d895-4452-d370-2a0f4fdafe43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Asymmetric noise_level_0_asy Accuracy: 52.35%, Loss: 1.3761\n",
            "Asymmetric noise_level_10_asy Accuracy: 51.03%, Loss: 1.4129\n",
            "Asymmetric noise_level_20_asy Accuracy: 50.45%, Loss: 1.4018\n",
            "Asymmetric noise_level_30_asy Accuracy: 49.74%, Loss: 1.4322\n",
            "Asymmetric noise_level_40_asy Accuracy: 44.24%, Loss: 1.5324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nDPybcSgY7t2"
      }
    }
  ]
}
