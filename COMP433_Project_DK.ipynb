{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suinkangme/comp433_project/blob/main/COMP433_Project_DK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Developing a robust CNN model to address the challenge of learning with label noise in  CIFAR10 dataset\n",
        "\n",
        "- CIFAR10 Label : ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’.\n",
        "\n",
        "- image size : 3x32x32\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TyD3ZYpdMfXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "ItN3Ur88Mbn1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training on GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "0CEWNvdcTCC1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and normalize CIFAR10"
      ],
      "metadata": {
        "id": "gt98-baGNJdN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rn4qMyOLcgV"
      },
      "outputs": [],
      "source": [
        "# transform_train = transforms.Compose([\n",
        "#     transforms.RandomResizedCrop(32),\n",
        "#     transforms.RandomHorizontalFlip(),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "# ])\n",
        "\n",
        "# transform_val = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "# ])\n",
        "\n",
        "# transform_test = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "# ])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the mean and standard deviation of CIFAR-10 dataset\n",
        "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
        "cifar10_std = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "# Training transformations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
        "])\n",
        "\n",
        "# Validation and testing transformations\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
        "])"
      ],
      "metadata": {
        "id": "_qbr2U9H1foB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download dataset for training\n",
        "cifar_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=None)\n",
        "\n",
        "# 데이터를 train과 validation으로 나누기 위해 인덱스 생성\n",
        "dataset_size = len(cifar_dataset)\n",
        "validation_split = 0.2\n",
        "val_size = int(validation_split * dataset_size)\n",
        "train_size = dataset_size - val_size\n",
        "\n",
        "# 데이터를 나누기\n",
        "train_dataset, val_dataset = random_split(cifar_dataset, [train_size, val_size])\n",
        "\n",
        "# 적절한 transform 적용\n",
        "train_dataset.dataset.transform = transform_train\n",
        "val_dataset.dataset.transform = transform_val\n",
        "\n",
        "# 데이터 로더 설정\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umTQUZh9V1xS",
        "outputId": "a52ff4ff-ad47-450b-b977-b2eefb91d289"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# test dataset\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                          train=False,\n",
        "                                          download=True,\n",
        "                                          transform = transform_test)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce3wRQHnNG9h",
        "outputId": "ea3967a4-a4f8-4d5f-b177-6865b44b3bdf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define & train with a  baseline CNN model"
      ],
      "metadata": {
        "id": "6nGN9QRnOfLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaselineModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BaselineModel,self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "      nn.Conv2d(3, 8, kernel_size=3, padding = 1),  # (input channel, output channels, kernel size, padding)  32*32*8\n",
        "      nn.ReLU(inplace=True), # activation function modifies the input tensor directly\n",
        "      nn.Conv2d(8, 16, kernel_size=3, padding=1),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=2,stride=2), # 16*16*16\n",
        "\n",
        "      nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "      nn.MaxPool2d(kernel_size=2,stride=2) # 8*8*128\n",
        "    )\n",
        "\n",
        "    # fully connected layers\n",
        "    self.fc_layers = nn.Sequential(\n",
        "      nn.Linear(128*8*8, 120),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Linear(120,84),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Linear(84,10)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = torch.flatten(x,1)\n",
        "    x = self.fc_layers(x)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "dOrsrTZ9M7wM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = BaselineModel().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(base_model.parameters(), lr=0.01, momentum=0.9)\n"
      ],
      "metadata": {
        "id": "sIhU5JCnOx6R"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.train()\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    base_model.train()\n",
        "    total_train_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        # Move data to GPU\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = base_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    average_train_loss = total_train_loss / len(train_loader)\n",
        "    train_acc = 100 * correct_train / total_train\n",
        "\n",
        "    # Validation\n",
        "    base_model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            # Move data to GPU\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = base_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    average_val_loss = total_val_loss / len(val_loader)\n",
        "    val_acc = 100 * correct_val / total_val\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
        "          f'Train Loss: {average_train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, '\n",
        "          f'Validation Loss: {average_val_loss:.4f}, Validation Accuracy: {val_acc:.2f}')\n"
      ],
      "metadata": {
        "id": "rZc3RsSbO2Qt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be8183db-246a-4168-abf3-0169b58a9f1b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 1.8797, Train Accuracy: 30.47%, Validation Loss: 1.4473, Validation Accuracy: 46.93\n",
            "Epoch 2/10, Train Loss: 1.3209, Train Accuracy: 52.56%, Validation Loss: 1.1206, Validation Accuracy: 59.38\n",
            "Epoch 3/10, Train Loss: 1.0377, Train Accuracy: 63.13%, Validation Loss: 0.9842, Validation Accuracy: 65.30\n",
            "Epoch 4/10, Train Loss: 0.8655, Train Accuracy: 69.50%, Validation Loss: 0.8609, Validation Accuracy: 69.64\n",
            "Epoch 5/10, Train Loss: 0.7238, Train Accuracy: 74.56%, Validation Loss: 0.8665, Validation Accuracy: 70.44\n",
            "Epoch 6/10, Train Loss: 0.6086, Train Accuracy: 78.30%, Validation Loss: 0.8313, Validation Accuracy: 72.25\n",
            "Epoch 7/10, Train Loss: 0.5048, Train Accuracy: 82.10%, Validation Loss: 0.8742, Validation Accuracy: 71.56\n",
            "Epoch 8/10, Train Loss: 0.4076, Train Accuracy: 85.45%, Validation Loss: 0.9367, Validation Accuracy: 71.90\n",
            "Epoch 9/10, Train Loss: 0.3343, Train Accuracy: 88.10%, Validation Loss: 1.0634, Validation Accuracy: 70.11\n",
            "Epoch 10/10, Train Loss: 0.2769, Train Accuracy: 90.27%, Validation Loss: 1.1719, Validation Accuracy: 70.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "base_model.eval()\n",
        "\n",
        "# Variables to store predictions and ground truth labels\n",
        "num_correct_predictions = 0\n",
        "total_num_predictions = 0\n",
        "test_loss = 0.0\n",
        "\n",
        "# Iterate over the test dataset\n",
        "with torch.no_grad():  # temporarily set all requires_grad flags to False\n",
        "    for inputs, labels in test_loader:\n",
        "        # Move data to GPU\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = base_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Compute loss and number of accurate predictions\n",
        "        test_loss += loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        num_correct_predictions += (preds == labels).sum().item()\n",
        "        total_num_predictions += labels.size(0)\n",
        "\n",
        "# Compute average test loss\n",
        "average_test_loss = test_loss / len(test_loader.dataset)\n",
        "\n",
        "# Compute accuracy percentage\n",
        "accuracy = (num_correct_predictions / total_num_predictions) * 100\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%, Average Test Loss: {average_test_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnhiYGcF2kG7",
        "outputId": "9a8d0b3d-c270-415f-f53a-092825f145bc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 70.55%, Average Test Loss: 0.0182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Noise Labeling\n",
        "- 5 different noise levels (10%,\n",
        "30%, 50%, 80%, 90%)"
      ],
      "metadata": {
        "id": "-U2PfzQbqjq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Symmetric label noise"
      ],
      "metadata": {
        "id": "xBci2_LerBLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_symmetric_noise(labels, epsilon):\n",
        "    num_labels = len(labels)\n",
        "    num_flips = int(epsilon * num_labels)\n",
        "\n",
        "    # choose the label to be flipped\n",
        "    flip_indices = np.random.choice(num_labels, num_flips, replace=True)\n",
        "\n",
        "    # filp the label\n",
        "    labels[flip_indices] = np.random.randint(0, 10, num_flips)\n",
        "\n",
        "    return labels"
      ],
      "metadata": {
        "id": "oFdDk4NVq7bt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Asymmetric label noise"
      ],
      "metadata": {
        "id": "9QPh92lorDFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flip_labels_asymmetrically(labels, epsilon):\n",
        "    flip_rules = {\n",
        "        9: 1,   # Truck to Automobile\n",
        "        2: 0,   # Bird to Airplane\n",
        "        4: 7,   # Deer to Horse\n",
        "        3: 5,   # Cat to Dog\n",
        "        5: 3,   # Dog to Cat\n",
        "    }\n",
        "\n",
        "    flipped_labels = []\n",
        "    for label in labels:\n",
        "        # Check if label flipping should occur based on epsilon\n",
        "        if np.random.random() < epsilon:\n",
        "            # Flip the label based on the flip_rules dictionary\n",
        "            flipped_label = flip_rules.get(label, label)\n",
        "            flipped_labels.append(flipped_label)\n",
        "        else:\n",
        "            # If no flipping, keep the original label\n",
        "            flipped_labels.append(label)\n",
        "\n",
        "    return np.array(flipped_labels)\n"
      ],
      "metadata": {
        "id": "9JbGzoKTq7f4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model with the noise labeling"
      ],
      "metadata": {
        "id": "FCwqkc3bPFcG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train with the symmetric noise labeling"
      ],
      "metadata": {
        "id": "5V9Glhks0h9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# noise_levels\n",
        "noise_levels = [0.1, 0.3, 0.5, 0.8, 0.9]\n",
        "\n",
        "# Create a dictionary with keys in the format 'noise_level_{100 * value}'\n",
        "symmetric_model_dict = {f'noise_level_{int(100 * level)}_sy': None for level in noise_levels}\n",
        "\n",
        "for epsilon in noise_levels:\n",
        "\n",
        "    net = BaselineModel()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "    net = net.to(device)\n",
        "    net.train()\n",
        "    num_epochs = 7\n",
        "\n",
        "    print(f\"Symmetric Training with noise level: {epsilon}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        net.train()\n",
        "        total_train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            labels_noisy = torch.from_numpy(replace_symmetric_noise(labels.numpy(), epsilon))\n",
        "\n",
        "            # Move data to GPU\n",
        "            inputs, labels_noisy = inputs.to(device), labels_noisy.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels_noisy)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels_noisy.size(0)\n",
        "            correct_train += (predicted == labels_noisy).sum().item()\n",
        "\n",
        "        average_train_loss = total_train_loss / len(train_loader)\n",
        "        train_acc = 100 * correct_train / total_train\n",
        "\n",
        "        # Validation\n",
        "        net.eval()\n",
        "        total_val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                # Move data to GPU\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = net(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                total_val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        average_val_loss = total_val_loss / len(val_loader)\n",
        "        val_acc = 100 * correct_val / total_val\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
        "              f'Train Loss: {average_train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, '\n",
        "              f'Validation Loss: {average_val_loss:.4f}, Validation Accuracy: {val_acc:.2f}')\n",
        "\n",
        "        # save model to dictionary\n",
        "        symmetric_model_dict[f'noise_level_{int(100 * epsilon)}_sy'] = net\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGlDJFFYZrD0",
        "outputId": "7accd72e-63d9-4367-f8dc-3278ff6ff871"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symmetric Training with noise level: 0.1\n",
            "Epoch 1/7, Train Loss: 2.0465, Train Accuracy: 24.59%, Validation Loss: 1.6988, Validation Accuracy: 39.54\n",
            "Epoch 2/7, Train Loss: 1.6100, Train Accuracy: 44.62%, Validation Loss: 1.2724, Validation Accuracy: 54.08\n",
            "Epoch 3/7, Train Loss: 1.4147, Train Accuracy: 53.60%, Validation Loss: 1.1544, Validation Accuracy: 60.34\n",
            "Epoch 4/7, Train Loss: 1.2629, Train Accuracy: 60.12%, Validation Loss: 1.0042, Validation Accuracy: 65.99\n",
            "Epoch 5/7, Train Loss: 1.1553, Train Accuracy: 64.68%, Validation Loss: 0.9017, Validation Accuracy: 69.29\n",
            "Epoch 6/7, Train Loss: 1.0660, Train Accuracy: 68.82%, Validation Loss: 0.8539, Validation Accuracy: 71.17\n",
            "Epoch 7/7, Train Loss: 0.9918, Train Accuracy: 72.17%, Validation Loss: 0.8692, Validation Accuracy: 71.36\n",
            "Symmetric Training with noise level: 0.3\n",
            "Epoch 1/7, Train Loss: 2.1463, Train Accuracy: 22.03%, Validation Loss: 1.7376, Validation Accuracy: 38.80\n",
            "Epoch 2/7, Train Loss: 1.9215, Train Accuracy: 34.79%, Validation Loss: 1.5292, Validation Accuracy: 47.33\n",
            "Epoch 3/7, Train Loss: 1.8121, Train Accuracy: 40.34%, Validation Loss: 1.4666, Validation Accuracy: 52.95\n",
            "Epoch 4/7, Train Loss: 1.7188, Train Accuracy: 45.38%, Validation Loss: 1.2760, Validation Accuracy: 58.65\n",
            "Epoch 5/7, Train Loss: 1.6501, Train Accuracy: 48.96%, Validation Loss: 1.2394, Validation Accuracy: 61.90\n",
            "Epoch 6/7, Train Loss: 1.5969, Train Accuracy: 52.09%, Validation Loss: 1.1427, Validation Accuracy: 65.21\n",
            "Epoch 7/7, Train Loss: 1.5447, Train Accuracy: 54.96%, Validation Loss: 1.1242, Validation Accuracy: 66.64\n",
            "Symmetric Training with noise level: 0.5\n",
            "Epoch 1/7, Train Loss: 2.2458, Train Accuracy: 16.09%, Validation Loss: 1.9359, Validation Accuracy: 35.09\n",
            "Epoch 2/7, Train Loss: 2.0925, Train Accuracy: 27.24%, Validation Loss: 1.7138, Validation Accuracy: 44.57\n",
            "Epoch 3/7, Train Loss: 2.0088, Train Accuracy: 32.48%, Validation Loss: 1.5332, Validation Accuracy: 50.78\n",
            "Epoch 4/7, Train Loss: 1.9551, Train Accuracy: 36.23%, Validation Loss: 1.4961, Validation Accuracy: 53.69\n",
            "Epoch 5/7, Train Loss: 1.9013, Train Accuracy: 39.30%, Validation Loss: 1.3547, Validation Accuracy: 58.78\n",
            "Epoch 6/7, Train Loss: 1.8655, Train Accuracy: 41.63%, Validation Loss: 1.3115, Validation Accuracy: 63.38\n",
            "Epoch 7/7, Train Loss: 1.8380, Train Accuracy: 43.20%, Validation Loss: 1.3278, Validation Accuracy: 63.46\n",
            "Symmetric Training with noise level: 0.8\n",
            "Epoch 1/7, Train Loss: 2.2753, Train Accuracy: 14.42%, Validation Loss: 2.1004, Validation Accuracy: 27.27\n",
            "Epoch 2/7, Train Loss: 2.2251, Train Accuracy: 19.42%, Validation Loss: 1.9233, Validation Accuracy: 37.65\n",
            "Epoch 3/7, Train Loss: 2.1777, Train Accuracy: 23.52%, Validation Loss: 1.9049, Validation Accuracy: 38.88\n",
            "Epoch 4/7, Train Loss: 2.1476, Train Accuracy: 25.78%, Validation Loss: 1.7637, Validation Accuracy: 46.64\n",
            "Epoch 5/7, Train Loss: 2.1244, Train Accuracy: 27.45%, Validation Loss: 1.6621, Validation Accuracy: 51.01\n",
            "Epoch 6/7, Train Loss: 2.1078, Train Accuracy: 29.14%, Validation Loss: 1.6006, Validation Accuracy: 54.34\n",
            "Epoch 7/7, Train Loss: 2.0884, Train Accuracy: 30.60%, Validation Loss: 1.5978, Validation Accuracy: 56.22\n",
            "Symmetric Training with noise level: 0.9\n",
            "Epoch 1/7, Train Loss: 2.2893, Train Accuracy: 12.64%, Validation Loss: 2.1508, Validation Accuracy: 23.62\n",
            "Epoch 2/7, Train Loss: 2.2408, Train Accuracy: 18.18%, Validation Loss: 1.9585, Validation Accuracy: 36.25\n",
            "Epoch 3/7, Train Loss: 2.2036, Train Accuracy: 21.48%, Validation Loss: 1.8831, Validation Accuracy: 40.69\n",
            "Epoch 4/7, Train Loss: 2.1787, Train Accuracy: 23.72%, Validation Loss: 1.7937, Validation Accuracy: 46.69\n",
            "Epoch 5/7, Train Loss: 2.1605, Train Accuracy: 25.45%, Validation Loss: 1.7700, Validation Accuracy: 48.56\n",
            "Epoch 6/7, Train Loss: 2.1420, Train Accuracy: 26.71%, Validation Loss: 1.6960, Validation Accuracy: 52.17\n",
            "Epoch 7/7, Train Loss: 2.1272, Train Accuracy: 28.19%, Validation Loss: 1.6460, Validation Accuracy: 56.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ob5nTG2F9GWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "5yXcz4Ye8fdB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xFLvD3uf9Fer"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, net in symmetric_model_dict.items():\n",
        "    net.eval()\n",
        "    net.to(device)\n",
        "\n",
        "    # Variables to store predictions and ground truth labels\n",
        "    num_correct_predictions = 0\n",
        "    total_num_predictions = 0\n",
        "    loss = 0.0\n",
        "\n",
        "    # Iterate over the test dataset\n",
        "    with torch.no_grad():  # temporarily set all requires_grad flags to False\n",
        "        for i, (data, label) in enumerate(test_loader):\n",
        "            # move inputs to desired device and dtype\n",
        "            data = data.to(device, dtype=torch.float32)\n",
        "            label = label.to(device, dtype=torch.long)\n",
        "\n",
        "            # forward pass\n",
        "            logit = net(data)\n",
        "\n",
        "            # compute loss and number of accurate predictions\n",
        "            loss += torch.nn.functional.cross_entropy(logit, label, reduction='sum').item()\n",
        "            preds = logit.max(dim=1)[1]\n",
        "            num_correct_predictions += (preds == label).sum().item()\n",
        "            total_num_predictions += len(preds)\n",
        "\n",
        "        # compute average loss\n",
        "        loss /= total_num_predictions\n",
        "\n",
        "        # compute accuracy percentage\n",
        "        accuracy = (float(num_correct_predictions) / total_num_predictions) * 100\n",
        "\n",
        "        print(f\"Symmetric {key} Accuracy: {accuracy:.2f}%, Loss: {loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "z0ax5iBOPY_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "677ceedb-ba1a-4f95-e76d-a0bb4deaf519"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Symmetric noise_level_10_sy Accuracy: 72.27%, Loss: 0.8703\n",
            "Symmetric noise_level_30_sy Accuracy: 67.47%, Loss: 1.1185\n",
            "Symmetric noise_level_50_sy Accuracy: 63.19%, Loss: 1.3321\n",
            "Symmetric noise_level_80_sy Accuracy: 56.61%, Loss: 1.5890\n",
            "Symmetric noise_level_90_sy Accuracy: 56.72%, Loss: 1.6384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train with the asymmetric noise labeling"
      ],
      "metadata": {
        "id": "8N3-1RaLih-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# noise_levels\n",
        "noise_levels = [0.1, 0.3, 0.5, 0.8, 0.9]\n",
        "# Create a dictionary with keys in the format 'noise_level_{100 * value}'\n",
        "asymmetric_model_dict = {f'noise_level_{int(100 * level)}_asy': None for level in noise_levels}\n",
        "\n",
        "\n",
        "\n",
        "# Training on GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "for epsilon in noise_levels:\n",
        "\n",
        "\n",
        "    net = BaselineModel()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum = 0.9)\n",
        "\n",
        "\n",
        "    net = net.to(device)\n",
        "    net.train()\n",
        "    num_epochs = 5\n",
        "\n",
        "\n",
        "    print(f\"Asymmetric Training with noise level: {epsilon}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      # Training\n",
        "      net.train()\n",
        "      total_train_loss = 0.0\n",
        "      correct_train = 0\n",
        "      total_train = 0\n",
        "\n",
        "      for inputs, labels in train_loader:\n",
        "          labels_noisy = torch.from_numpy(flip_labels_asymmetrically(labels.numpy(), epsilon))\n",
        "          # Move data to GPU\n",
        "          inputs, labels_noisy = inputs.to(device), labels_noisy.to(device)\n",
        "\n",
        "          # Zero the gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = net(inputs)\n",
        "          loss = criterion(outputs, labels_noisy)\n",
        "\n",
        "          # Backward pass and optimization\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          total_train_loss += loss.item()\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total_train += labels_noisy.size(0)\n",
        "          correct_train += (predicted == labels_noisy).sum().item()\n",
        "\n",
        "      average_train_loss = total_train_loss / len(train_loader)\n",
        "      train_acc = 100 * correct_train / total_train\n",
        "\n",
        "      # Validation\n",
        "      net.eval()\n",
        "      total_val_loss = 0.0\n",
        "      correct_val = 0\n",
        "      total_val = 0\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for inputs, labels in val_loader:\n",
        "              # Move data to GPU\n",
        "              inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "              # Forward pass\n",
        "              outputs = net(inputs)\n",
        "              loss = criterion(outputs, labels)\n",
        "\n",
        "              total_val_loss += loss.item()\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              total_val += labels.size(0)\n",
        "              correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "      average_val_loss = total_val_loss / len(val_loader)\n",
        "      val_acc = 100 * correct_val / total_val\n",
        "\n",
        "      print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
        "            f'Train Loss: {average_train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, '\n",
        "            f'Validation Loss: {average_val_loss:.4f}, Validation Accuracy: {val_acc:.2f}')\n",
        "    # save model to dictionary\n",
        "    asymmetric_model_dict[f'noise_level_{int(100 * epsilon)}_asy'] = net\n"
      ],
      "metadata": {
        "id": "zRiBGV6bPf6n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1a52a55-d1ad-4ab3-af0b-c5b958d5ecc3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Asymmetric Training with noise level: 0.1\n",
            "Epoch 1/5, Train Loss: 1.9821, Train Accuracy: 26.18%, Validation Loss: 1.7411, Validation Accuracy: 37.74\n",
            "Epoch 2/5, Train Loss: 1.4303, Train Accuracy: 48.19%, Validation Loss: 1.2325, Validation Accuracy: 55.63\n",
            "Epoch 3/5, Train Loss: 1.1628, Train Accuracy: 58.32%, Validation Loss: 1.1088, Validation Accuracy: 61.14\n",
            "Epoch 4/5, Train Loss: 0.9861, Train Accuracy: 64.75%, Validation Loss: 0.9476, Validation Accuracy: 67.11\n",
            "Epoch 5/5, Train Loss: 0.8570, Train Accuracy: 69.65%, Validation Loss: 0.8520, Validation Accuracy: 70.67\n",
            "Asymmetric Training with noise level: 0.3\n",
            "Epoch 1/5, Train Loss: 1.9450, Train Accuracy: 28.71%, Validation Loss: 1.7167, Validation Accuracy: 35.70\n",
            "Epoch 2/5, Train Loss: 1.4159, Train Accuracy: 47.48%, Validation Loss: 1.2746, Validation Accuracy: 51.81\n",
            "Epoch 3/5, Train Loss: 1.1787, Train Accuracy: 55.98%, Validation Loss: 1.1225, Validation Accuracy: 59.69\n",
            "Epoch 4/5, Train Loss: 1.0312, Train Accuracy: 60.50%, Validation Loss: 1.0074, Validation Accuracy: 64.98\n",
            "Epoch 5/5, Train Loss: 0.9254, Train Accuracy: 63.87%, Validation Loss: 0.9172, Validation Accuracy: 68.96\n",
            "Asymmetric Training with noise level: 0.5\n",
            "Epoch 1/5, Train Loss: 1.9406, Train Accuracy: 29.42%, Validation Loss: 1.7558, Validation Accuracy: 33.96\n",
            "Epoch 2/5, Train Loss: 1.4038, Train Accuracy: 48.09%, Validation Loss: 1.2894, Validation Accuracy: 48.70\n",
            "Epoch 3/5, Train Loss: 1.1734, Train Accuracy: 55.05%, Validation Loss: 1.2016, Validation Accuracy: 50.58\n",
            "Epoch 4/5, Train Loss: 1.0433, Train Accuracy: 58.62%, Validation Loss: 1.1155, Validation Accuracy: 54.50\n",
            "Epoch 5/5, Train Loss: 0.9360, Train Accuracy: 61.43%, Validation Loss: 1.1758, Validation Accuracy: 50.86\n",
            "Asymmetric Training with noise level: 0.8\n",
            "Epoch 1/5, Train Loss: 1.8542, Train Accuracy: 30.23%, Validation Loss: 1.9557, Validation Accuracy: 32.57\n",
            "Epoch 2/5, Train Loss: 1.3713, Train Accuracy: 50.96%, Validation Loss: 1.7976, Validation Accuracy: 38.14\n",
            "Epoch 3/5, Train Loss: 1.1448, Train Accuracy: 59.65%, Validation Loss: 1.5476, Validation Accuracy: 40.62\n",
            "Epoch 4/5, Train Loss: 0.9906, Train Accuracy: 65.19%, Validation Loss: 1.5934, Validation Accuracy: 43.66\n",
            "Epoch 5/5, Train Loss: 0.8758, Train Accuracy: 69.09%, Validation Loss: 1.4108, Validation Accuracy: 44.67\n",
            "Asymmetric Training with noise level: 0.9\n",
            "Epoch 1/5, Train Loss: 1.7454, Train Accuracy: 33.77%, Validation Loss: 2.3286, Validation Accuracy: 30.75\n",
            "Epoch 2/5, Train Loss: 1.2435, Train Accuracy: 55.19%, Validation Loss: 1.8162, Validation Accuracy: 40.33\n",
            "Epoch 3/5, Train Loss: 1.0195, Train Accuracy: 64.18%, Validation Loss: 1.7541, Validation Accuracy: 42.65\n",
            "Epoch 4/5, Train Loss: 0.8852, Train Accuracy: 69.30%, Validation Loss: 1.7326, Validation Accuracy: 43.45\n",
            "Epoch 5/5, Train Loss: 0.7879, Train Accuracy: 72.61%, Validation Loss: 1.6356, Validation Accuracy: 44.80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "FD4NBMyA8X4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key, net in asymmetric_model_dict.items():\n",
        "    net.eval()\n",
        "    net.to(device)\n",
        "\n",
        "    # Variables to store predictions and ground truth labels\n",
        "    num_correct_predictions = 0\n",
        "    total_num_predictions = 0\n",
        "    loss = 0.0\n",
        "\n",
        "    # Iterate over the test dataset\n",
        "    with torch.no_grad():  # temporarily set all requires_grad flags to False\n",
        "        for i, (data, label) in enumerate(test_loader):\n",
        "            # move inputs to desired device and dtype\n",
        "            data = data.to(device, dtype=torch.float32)\n",
        "            label = label.to(device, dtype=torch.long)\n",
        "\n",
        "            # forward pass\n",
        "            logit = net(data)\n",
        "\n",
        "            # compute loss and number of accurate predictions\n",
        "            loss += torch.nn.functional.cross_entropy(logit, label, reduction='sum').item()\n",
        "            preds = logit.max(dim=1)[1]\n",
        "            num_correct_predictions += (preds == label).sum().item()\n",
        "            total_num_predictions += len(preds)\n",
        "\n",
        "        # compute average loss\n",
        "        loss /= total_num_predictions\n",
        "\n",
        "        # compute accuracy percentage\n",
        "        accuracy = (float(num_correct_predictions) / total_num_predictions) * 100\n",
        "\n",
        "        print(f\"Asymmetric {key} Accuracy: {accuracy:.2f}%, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "id": "PkWK7f63Pf8w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "094057a8-e4ac-44ad-bd45-632163a96afd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Asymmetric noise_level_10_asy Accuracy: 70.21%, Loss: 0.8703\n",
            "Asymmetric noise_level_30_asy Accuracy: 68.22%, Loss: 0.9198\n",
            "Asymmetric noise_level_50_asy Accuracy: 52.25%, Loss: 1.1821\n",
            "Asymmetric noise_level_80_asy Accuracy: 45.68%, Loss: 1.3927\n",
            "Asymmetric noise_level_90_asy Accuracy: 45.46%, Loss: 1.6320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Active Passive Loss (APL)"
      ],
      "metadata": {
        "id": "dpkHZMy3iS6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Active Loss Functions**:\n",
        "<br> Cross Entropy (CE)\n",
        "<br> Normalized Cross Entropy (NCE)\n",
        "<br> Focal Loss (FL)\n",
        "<br> Normalized Focal Loss (NFL)\n",
        "<br>\n",
        "\n",
        "\n",
        "<br> **2. Passive Loss Functions**:\n",
        "<br> Mean Absolute Error (MAE)\n",
        "<br> Normalized Mean Absolute Error (NMAE)\n",
        "<br> Reverse Cross Entropy (RCE)\n",
        "<br> Normalized Reverse Cross Entropy (NRCE)\n",
        "<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "PWwvdFdOpL-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class APLLoss(nn.Module):\n",
        "    def __init__(self, alpha, beta):\n",
        "        super(APLLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.active_loss = nn.CrossEntropyLoss()  # CE\n",
        "        self.passive_loss = nn.L1Loss()  # MAE\n",
        "\n",
        "    def forward(self, outputs, labels):\n",
        "        active_loss = self.active_loss(outputs, labels)\n",
        "\n",
        "        # label_one_hot = torch.nn.functional.one_hot(labels.clone().detach(), 10)\n",
        "        # loss_passive = self.passive_loss(outputs, label_one_hot.view(-1, 1))\n",
        "        # loss_passive = self.passive_loss(outputs, label_one_hot)\n",
        "         # 수동적인 부분: 평균 절대 오차\n",
        "        target_one_hot = F.one_hot(labels, num_classes=outputs.size(1)).float()\n",
        "        passive_loss = F.l1_loss(F.softmax(outputs, dim=1), target_one_hot)\n",
        "\n",
        "        apl_loss = (self.alpha * active_loss) + (self.beta * passive_loss)\n",
        "        return apl_loss\n"
      ],
      "metadata": {
        "id": "Nw9CZ_AQiSrN"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BEyp2RiEMzgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.5  # 이 값은 조절 가능\n",
        "beta = 0.5   # 이 값은 조절 가능\n",
        "apl_criterion = APLLoss(alpha, beta)\n",
        "\n"
      ],
      "metadata": {
        "id": "OG-Omj2joD9E"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "APL_model = BaselineModel().to(device)\n",
        "optimizer = optim.SGD(APL_model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "APL_model.train()\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    APL_model.train()\n",
        "    total_train_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        # Move data to GPU\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = APL_model(inputs)\n",
        "        loss = apl_criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    average_train_loss = total_train_loss / len(train_loader)\n",
        "    train_acc = 100 * correct_train / total_train\n",
        "\n",
        "    # Validation\n",
        "    APL_model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            # Move data to GPU\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = APL_model(inputs)\n",
        "            loss = apl_criterion(outputs, labels)\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    average_val_loss = total_val_loss / len(val_loader)\n",
        "    val_acc = 100 * correct_val / total_val\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
        "          f'Train Loss: {average_train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, '\n",
        "          f'Validation Loss: {average_val_loss:.4f}, Validation Accuracy: {val_acc:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PooGP9M-uLp-",
        "outputId": "3bdd0276-37f5-4e47-e361-3bfd97f71402"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 1.1233, Train Accuracy: 22.99%, Validation Loss: 0.9355, Validation Accuracy: 38.27\n",
            "Epoch 2/5, Train Loss: 0.8480, Train Accuracy: 43.63%, Validation Loss: 0.7786, Validation Accuracy: 48.16\n",
            "Epoch 3/5, Train Loss: 0.7153, Train Accuracy: 52.82%, Validation Loss: 0.7105, Validation Accuracy: 53.55\n",
            "Epoch 4/5, Train Loss: 0.5988, Train Accuracy: 60.85%, Validation Loss: 0.5779, Validation Accuracy: 62.70\n",
            "Epoch 5/5, Train Loss: 0.5089, Train Accuracy: 67.23%, Validation Loss: 0.5488, Validation Accuracy: 65.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "1ckW2OXWFVe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "APL_model.eval()\n",
        "\n",
        "# Variables to store predictions and ground truth labels\n",
        "num_correct_predictions = 0\n",
        "total_num_predictions = 0\n",
        "test_loss = 0.0\n",
        "\n",
        "# Iterate over the test dataset\n",
        "with torch.no_grad():  # temporarily set all requires_grad flags to False\n",
        "    for inputs, labels in test_loader:\n",
        "        # Move data to GPU\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = APL_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Compute loss and number of accurate predictions\n",
        "        test_loss += loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        num_correct_predictions += (preds == labels).sum().item()\n",
        "        total_num_predictions += labels.size(0)\n",
        "\n",
        "# Compute average test loss\n",
        "average_test_loss = test_loss / len(test_loader.dataset)\n",
        "\n",
        "# Compute accuracy percentage\n",
        "accuracy = (num_correct_predictions / total_num_predictions) * 100\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%, Average Test Loss: {average_test_loss:.4f}\")"
      ],
      "metadata": {
        "id": "Zz_Aom6Rxifz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f28bd9e8-6370-4980-9237-5491c72ebfeb"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 57.62%, Average Test Loss: 0.0187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## APL with noisy label 90"
      ],
      "metadata": {
        "id": "szYoWjPhb6Mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.6  # 이 값은 조절 가능\n",
        "beta = 0.4   # 이 값은 조절 가능\n",
        "apl_criterion = APLLoss(alpha, beta)\n",
        "\n",
        "APL_model = BaselineModel().to(device)\n",
        "optimizer = optim.SGD(APL_model.parameters(), lr=0.01, momentum=0.9)\n"
      ],
      "metadata": {
        "id": "cIOeDyhbcBlz"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "APL_model.train()\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    APL_model.train()\n",
        "    total_train_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "\n",
        "        labels_noisy = torch.tensor(flip_labels_asymmetrically(labels.cpu().numpy(), 0.9)).to(device)\n",
        "        # Move data to GPU\n",
        "        inputs, labels_noisy = inputs.to(device), labels_noisy.to(device)\n",
        "\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = APL_model(inputs)\n",
        "        loss = apl_criterion(outputs, labels_noisy)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels_noisy.size(0)\n",
        "        correct_train += (predicted == labels_noisy).sum().item()\n",
        "\n",
        "    average_train_loss = total_train_loss / len(train_loader)\n",
        "    train_acc = 100 * correct_train / total_train\n",
        "\n",
        "    # Validation\n",
        "    APL_model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            # Move data to GPU\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = APL_model(inputs)\n",
        "            loss = apl_criterion(outputs, labels)\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    average_val_loss = total_val_loss / len(val_loader)\n",
        "    val_acc = 100 * correct_val / total_val\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
        "          f'Train Loss: {average_train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, '\n",
        "          f'Validation Loss: {average_val_loss:.4f}, Validation Accuracy: {val_acc:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPFdFTmjcBjD",
        "outputId": "55e9a931-9d72-4eb3-cdac-e5f4873a4981"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 1.1213, Train Accuracy: 35.29%, Validation Loss: 1.3728, Validation Accuracy: 31.44\n",
            "Epoch 2/10, Train Loss: 0.8426, Train Accuracy: 51.95%, Validation Loss: 1.2349, Validation Accuracy: 38.98\n",
            "Epoch 3/10, Train Loss: 0.6976, Train Accuracy: 61.11%, Validation Loss: 1.1579, Validation Accuracy: 41.93\n",
            "Epoch 4/10, Train Loss: 0.6113, Train Accuracy: 66.33%, Validation Loss: 1.1006, Validation Accuracy: 42.87\n",
            "Epoch 5/10, Train Loss: 0.5426, Train Accuracy: 70.47%, Validation Loss: 1.1513, Validation Accuracy: 42.92\n",
            "Epoch 6/10, Train Loss: 0.4883, Train Accuracy: 73.56%, Validation Loss: 1.0417, Validation Accuracy: 44.17\n",
            "Epoch 7/10, Train Loss: 0.4408, Train Accuracy: 76.27%, Validation Loss: 1.1111, Validation Accuracy: 45.08\n",
            "Epoch 8/10, Train Loss: 0.3961, Train Accuracy: 78.55%, Validation Loss: 0.9961, Validation Accuracy: 45.24\n",
            "Epoch 9/10, Train Loss: 0.3418, Train Accuracy: 81.70%, Validation Loss: 1.0783, Validation Accuracy: 44.80\n",
            "Epoch 10/10, Train Loss: 0.3048, Train Accuracy: 83.76%, Validation Loss: 1.0948, Validation Accuracy: 45.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "TgUkaN7zIKnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "APL_model.eval()\n",
        "\n",
        "# Variables to store predictions and ground truth labels\n",
        "num_correct_predictions = 0\n",
        "total_num_predictions = 0\n",
        "test_loss = 0.0\n",
        "\n",
        "# Iterate over the test dataset\n",
        "with torch.no_grad():  # temporarily set all requires_grad flags to False\n",
        "    for inputs, labels in test_loader:\n",
        "        # Move data to GPU\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = APL_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Compute loss and number of accurate predictions\n",
        "        test_loss += loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        num_correct_predictions += (preds == labels).sum().item()\n",
        "        total_num_predictions += labels.size(0)\n",
        "\n",
        "# Compute average test loss\n",
        "average_test_loss = test_loss / len(test_loader.dataset)\n",
        "\n",
        "# Compute accuracy percentage\n",
        "accuracy = (num_correct_predictions / total_num_predictions) * 100\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%, Average Test Loss: {average_test_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7r2yRtanIM7A",
        "outputId": "31b5e8a1-db88-4e7f-90be-343efc13193c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 43.42%, Average Test Loss: 0.0285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Different noises"
      ],
      "metadata": {
        "id": "hCM0LA05Rvaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def passive_loss_mae(outputs, targets):\n",
        "    return F.l1_loss(outputs, targets)\n"
      ],
      "metadata": {
        "id": "iTJ6Zw2qjco-"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def passive_loss_nmae(outputs, targets, epsilon=1e-8):\n",
        "    # 절대 에러 계산\n",
        "    absolute_errors = torch.abs(outputs - targets)\n",
        "\n",
        "    # 절대 에러의 평균 계산\n",
        "    absolute_mean = torch.mean(absolute_errors)\n",
        "\n",
        "    # 타겟 범위 계산\n",
        "    target_range = torch.max(targets) - torch.min(targets)\n",
        "\n",
        "    # 정규화된 평균 절대 에러 계산\n",
        "    return absolute_mean / (target_range + epsilon)"
      ],
      "metadata": {
        "id": "8Q9mokGOR1tM"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q0G26EglR6B0"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def passive_loss_nrce(outputs, targets, epsilon=1e-8):\n",
        "    # 소프트맥스 함수를 사용하여 예측 확률 계산\n",
        "    pred_prob = F.softmax(outputs, dim=1).to(outputs.device)\n",
        "\n",
        "    # 정답의 원-핫 인코딩 생성\n",
        "    true_prob = F.one_hot(targets, num_classes=outputs.size(1)).float().to(outputs.device)\n",
        "\n",
        "    # 교차 엔트로피 계산\n",
        "    cross_entropy = -torch.sum(true_prob * torch.log(pred_prob + epsilon), dim=1)\n",
        "\n",
        "    # 예측의 엔트로피 계산\n",
        "    entropy = -torch.sum(pred_prob * torch.log(pred_prob + epsilon), dim=1)\n",
        "\n",
        "    # 정규화된 평균 교차 엔트로피 계산\n",
        "    nrce = torch.mean(cross_entropy / entropy)\n",
        "\n",
        "    return nrce"
      ],
      "metadata": {
        "id": "pwEyS-eTV9wu"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class APLLoss(nn.Module):\n",
        "    def __init__(self, alpha, beta):\n",
        "        super(APLLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.active_loss = nn.CrossEntropyLoss()  # CE\n",
        "\n",
        "    def passive_loss(self, outputs, labels):\n",
        "        return passive_loss_nrce(outputs, labels)\n",
        "\n",
        "    def forward(self, outputs, labels):\n",
        "        active_loss = self.active_loss(outputs, labels)\n",
        "        passive_loss = self.passive_loss(outputs, labels)\n",
        "\n",
        "        apl_loss = (self.alpha * active_loss) + (self.beta * passive_loss)\n",
        "        return apl_loss"
      ],
      "metadata": {
        "id": "XOcVUAOJWDTN"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.2 # 이 값은 조절 가능\n",
        "beta =  0.8   # 이 값은 조절 가능\n",
        "apl_criterion = APLLoss(alpha, beta)\n",
        "\n",
        "APL_model = BaselineModel().to(device)\n",
        "optimizer = optim.SGD(APL_model.parameters(), lr=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "41akBpvyWDN8"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "APL_model.train()\n",
        "num_epochs = 7\n",
        "\n",
        "# noise_levels\n",
        "noise_levels = [0.1, 0.3, 0.5, 0.8, 0.9]\n",
        "# Create a dictionary with keys in the format 'noise_level_{100 * value}'\n",
        "asymmetric_model_dict = {f'noise_level_{int(100 * level)}_asy': None for level in noise_levels}\n",
        "\n",
        "\n",
        "\n",
        "# Training on GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "for epsilon in noise_levels:\n",
        "\n",
        "\n",
        "    net = BaselineModel()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum = 0.9)\n",
        "\n",
        "\n",
        "    net = net.to(device)\n",
        "    net.train()\n",
        "\n",
        "\n",
        "    print(f\"Asymmetric Training with noise level: {epsilon}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      # Training\n",
        "      net.train()\n",
        "      total_train_loss = 0.0\n",
        "      correct_train = 0\n",
        "      total_train = 0\n",
        "\n",
        "      for inputs, labels in train_loader:\n",
        "          labels_noisy = torch.from_numpy(flip_labels_asymmetrically(labels.numpy(), epsilon))\n",
        "          # Move data to GPU\n",
        "          inputs, labels_noisy = inputs.to(device), labels_noisy.to(device)\n",
        "\n",
        "          # Zero the gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = net(inputs)\n",
        "          loss = criterion(outputs, labels_noisy)\n",
        "\n",
        "          # Backward pass and optimization\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          total_train_loss += loss.item()\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total_train += labels_noisy.size(0)\n",
        "          correct_train += (predicted == labels_noisy).sum().item()\n",
        "\n",
        "      average_train_loss = total_train_loss / len(train_loader)\n",
        "      train_acc = 100 * correct_train / total_train\n",
        "\n",
        "      # Validation\n",
        "      net.eval()\n",
        "      total_val_loss = 0.0\n",
        "      correct_val = 0\n",
        "      total_val = 0\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for inputs, labels in val_loader:\n",
        "              # Move data to GPU\n",
        "              inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "              # Forward pass\n",
        "              outputs = net(inputs)\n",
        "              loss = criterion(outputs, labels)\n",
        "\n",
        "              total_val_loss += loss.item()\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              total_val += labels.size(0)\n",
        "              correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "      average_val_loss = total_val_loss / len(val_loader)\n",
        "      val_acc = 100 * correct_val / total_val\n",
        "\n",
        "      print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
        "            f'Train Loss: {average_train_loss:.4f}, Train Accuracy: {train_acc:.2f}%, '\n",
        "            f'Validation Loss: {average_val_loss:.4f}, Validation Accuracy: {val_acc:.2f}')\n",
        "    # save model to dictionary\n",
        "    asymmetric_model_dict[f'noise_level_{int(100 * epsilon)}_asy'] = net"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4V3Wh87WkmL",
        "outputId": "1f757199-1bbf-424a-d633-3fbb775d45ea"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Asymmetric Training with noise level: 0.1\n",
            "Epoch 1/7, Train Loss: 2.0170, Train Accuracy: 24.98%, Validation Loss: 1.6364, Validation Accuracy: 40.35\n",
            "Epoch 2/7, Train Loss: 1.4791, Train Accuracy: 46.33%, Validation Loss: 1.2762, Validation Accuracy: 54.48\n",
            "Epoch 3/7, Train Loss: 1.2203, Train Accuracy: 56.20%, Validation Loss: 1.1077, Validation Accuracy: 60.58\n",
            "Epoch 4/7, Train Loss: 1.0352, Train Accuracy: 62.85%, Validation Loss: 0.9695, Validation Accuracy: 66.21\n",
            "Epoch 5/7, Train Loss: 0.8953, Train Accuracy: 68.22%, Validation Loss: 0.8907, Validation Accuracy: 69.17\n",
            "Epoch 6/7, Train Loss: 0.7768, Train Accuracy: 72.15%, Validation Loss: 0.8735, Validation Accuracy: 69.97\n",
            "Epoch 7/7, Train Loss: 0.6722, Train Accuracy: 76.17%, Validation Loss: 0.8649, Validation Accuracy: 71.21\n",
            "Asymmetric Training with noise level: 0.3\n",
            "Epoch 1/7, Train Loss: 2.0277, Train Accuracy: 24.68%, Validation Loss: 1.6193, Validation Accuracy: 39.81\n",
            "Epoch 2/7, Train Loss: 1.5138, Train Accuracy: 44.33%, Validation Loss: 1.3537, Validation Accuracy: 51.11\n",
            "Epoch 3/7, Train Loss: 1.2757, Train Accuracy: 52.54%, Validation Loss: 1.2146, Validation Accuracy: 56.66\n",
            "Epoch 4/7, Train Loss: 1.1069, Train Accuracy: 58.29%, Validation Loss: 1.0314, Validation Accuracy: 63.89\n",
            "Epoch 5/7, Train Loss: 0.9784, Train Accuracy: 62.15%, Validation Loss: 0.9829, Validation Accuracy: 63.84\n",
            "Epoch 6/7, Train Loss: 0.8648, Train Accuracy: 65.59%, Validation Loss: 0.9426, Validation Accuracy: 67.32\n",
            "Epoch 7/7, Train Loss: 0.7742, Train Accuracy: 68.24%, Validation Loss: 0.9629, Validation Accuracy: 66.97\n",
            "Asymmetric Training with noise level: 0.5\n",
            "Epoch 1/7, Train Loss: 1.9862, Train Accuracy: 27.33%, Validation Loss: 1.7160, Validation Accuracy: 35.74\n",
            "Epoch 2/7, Train Loss: 1.4466, Train Accuracy: 46.56%, Validation Loss: 1.3931, Validation Accuracy: 45.30\n",
            "Epoch 3/7, Train Loss: 1.2151, Train Accuracy: 53.86%, Validation Loss: 1.2982, Validation Accuracy: 49.98\n",
            "Epoch 4/7, Train Loss: 1.0727, Train Accuracy: 57.54%, Validation Loss: 1.2476, Validation Accuracy: 48.37\n",
            "Epoch 5/7, Train Loss: 0.9525, Train Accuracy: 61.31%, Validation Loss: 1.0855, Validation Accuracy: 52.02\n",
            "Epoch 6/7, Train Loss: 0.8656, Train Accuracy: 63.39%, Validation Loss: 1.0647, Validation Accuracy: 54.72\n",
            "Epoch 7/7, Train Loss: 0.7791, Train Accuracy: 65.54%, Validation Loss: 1.0759, Validation Accuracy: 56.28\n",
            "Asymmetric Training with noise level: 0.8\n",
            "Epoch 1/7, Train Loss: 1.7987, Train Accuracy: 33.30%, Validation Loss: 1.9475, Validation Accuracy: 35.48\n",
            "Epoch 2/7, Train Loss: 1.2841, Train Accuracy: 54.05%, Validation Loss: 1.6677, Validation Accuracy: 40.97\n",
            "Epoch 3/7, Train Loss: 1.0812, Train Accuracy: 61.94%, Validation Loss: 1.5425, Validation Accuracy: 43.87\n",
            "Epoch 4/7, Train Loss: 0.9438, Train Accuracy: 66.55%, Validation Loss: 1.3945, Validation Accuracy: 45.20\n",
            "Epoch 5/7, Train Loss: 0.8410, Train Accuracy: 70.46%, Validation Loss: 1.3895, Validation Accuracy: 45.94\n",
            "Epoch 6/7, Train Loss: 0.7530, Train Accuracy: 73.18%, Validation Loss: 1.4592, Validation Accuracy: 45.55\n",
            "Epoch 7/7, Train Loss: 0.6762, Train Accuracy: 75.43%, Validation Loss: 1.3827, Validation Accuracy: 45.27\n",
            "Asymmetric Training with noise level: 0.9\n",
            "Epoch 1/7, Train Loss: 1.7315, Train Accuracy: 34.57%, Validation Loss: 2.2949, Validation Accuracy: 31.96\n",
            "Epoch 2/7, Train Loss: 1.2794, Train Accuracy: 53.90%, Validation Loss: 1.9704, Validation Accuracy: 38.56\n",
            "Epoch 3/7, Train Loss: 1.0700, Train Accuracy: 62.20%, Validation Loss: 1.8193, Validation Accuracy: 42.05\n",
            "Epoch 4/7, Train Loss: 0.9249, Train Accuracy: 67.40%, Validation Loss: 1.7795, Validation Accuracy: 43.20\n",
            "Epoch 5/7, Train Loss: 0.8121, Train Accuracy: 71.93%, Validation Loss: 1.7309, Validation Accuracy: 44.62\n",
            "Epoch 6/7, Train Loss: 0.7116, Train Accuracy: 75.31%, Validation Loss: 1.8032, Validation Accuracy: 44.51\n",
            "Epoch 7/7, Train Loss: 0.6293, Train Accuracy: 78.42%, Validation Loss: 1.7286, Validation Accuracy: 43.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, net in asymmetric_model_dict.items():\n",
        "    net.eval()\n",
        "    net.to(device)\n",
        "\n",
        "    # Variables to store predictions and ground truth labels\n",
        "    num_correct_predictions = 0\n",
        "    total_num_predictions = 0\n",
        "    loss = 0.0\n",
        "\n",
        "    # Iterate over the test dataset\n",
        "    with torch.no_grad():  # temporarily set all requires_grad flags to False\n",
        "        for i, (data, label) in enumerate(test_loader):\n",
        "            # move inputs to desired device and dtype\n",
        "            data = data.to(device, dtype=torch.float32)\n",
        "            label = label.to(device, dtype=torch.long)\n",
        "\n",
        "            # forward pass\n",
        "            logit = net(data)\n",
        "\n",
        "            # compute loss and number of accurate predictions\n",
        "            loss += torch.nn.functional.cross_entropy(logit, label, reduction='sum').item()\n",
        "            preds = logit.max(dim=1)[1]\n",
        "            num_correct_predictions += (preds == label).sum().item()\n",
        "            total_num_predictions += len(preds)\n",
        "\n",
        "        # compute average loss\n",
        "        loss /= total_num_predictions\n",
        "\n",
        "        # compute accuracy percentage\n",
        "        accuracy = (float(num_correct_predictions) / total_num_predictions) * 100\n",
        "\n",
        "        print(f\"Asymmetric {key} Accuracy: {accuracy:.2f}%, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fINrjk5cwez",
        "outputId": "11f2effc-7084-49c7-da7a-56620f142fae"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Asymmetric noise_level_10_asy Accuracy: 70.57%, Loss: 0.8655\n",
            "Asymmetric noise_level_30_asy Accuracy: 66.17%, Loss: 0.9777\n",
            "Asymmetric noise_level_50_asy Accuracy: 56.64%, Loss: 1.0771\n",
            "Asymmetric noise_level_80_asy Accuracy: 45.37%, Loss: 1.3952\n",
            "Asymmetric noise_level_90_asy Accuracy: 43.99%, Loss: 1.7458\n"
          ]
        }
      ]
    }
  ]
}